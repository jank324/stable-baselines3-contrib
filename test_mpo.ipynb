{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sb3_contrib import MPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'BipedalWalker-v3'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./logs/MPO_15\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 454       |\n",
      "|    ep_rew_mean         | -99.2     |\n",
      "| time/                  |           |\n",
      "|    episodes            | 4         |\n",
      "|    fps                 | 81        |\n",
      "|    time_elapsed        | 22        |\n",
      "|    total_timesteps     | 1814      |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 11.1      |\n",
      "|    alpha_mean          | 1.15      |\n",
      "|    alpha_mean_loss     | 0.0447    |\n",
      "|    alpha_std           | 10.1      |\n",
      "|    alpha_std_loss      | -6.33e-05 |\n",
      "|    critic_loss         | 1.38      |\n",
      "|    dual_loss           | -0.232    |\n",
      "|    kl_loss             | 0.00142   |\n",
      "|    kl_mean_loss        | 0.00131   |\n",
      "|    kl_std_loss         | 0.000104  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 1600      |\n",
      "|    penalty_temperature | 1.47      |\n",
      "|    policy_loss         | 11.4      |\n",
      "|    policy_mean_loss    | 5.69      |\n",
      "|    policy_std_loss     | 5.69      |\n",
      "|    std                 | 0.493     |\n",
      "|    temperature         | 1.16      |\n",
      "|    temperature_loss    | -0.277    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 462       |\n",
      "|    ep_rew_mean         | -100      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 8         |\n",
      "|    fps                 | 76        |\n",
      "|    time_elapsed        | 48        |\n",
      "|    total_timesteps     | 3694      |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 8.03      |\n",
      "|    alpha_mean          | 0.729     |\n",
      "|    alpha_mean_loss     | 0.0235    |\n",
      "|    alpha_std           | 10.3      |\n",
      "|    alpha_std_loss      | -3.11e-05 |\n",
      "|    critic_loss         | 4.7       |\n",
      "|    dual_loss           | -3.25     |\n",
      "|    kl_loss             | 0.00577   |\n",
      "|    kl_mean_loss        | 0.0057    |\n",
      "|    kl_std_loss         | 7.23e-05  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 3450      |\n",
      "|    penalty_temperature | 1.96      |\n",
      "|    policy_loss         | 11.3      |\n",
      "|    policy_mean_loss    | 5.63      |\n",
      "|    policy_std_loss     | 5.64      |\n",
      "|    std                 | 0.491     |\n",
      "|    temperature         | 0.835     |\n",
      "|    temperature_loss    | -3.27     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 356       |\n",
      "|    ep_rew_mean         | -103      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 12        |\n",
      "|    fps                 | 74        |\n",
      "|    time_elapsed        | 57        |\n",
      "|    total_timesteps     | 4269      |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 8.18      |\n",
      "|    alpha_mean          | 0.663     |\n",
      "|    alpha_mean_loss     | 0.0191    |\n",
      "|    alpha_std           | 10.5      |\n",
      "|    alpha_std_loss      | -6.14e-05 |\n",
      "|    critic_loss         | 3.06      |\n",
      "|    dual_loss           | -3.12     |\n",
      "|    kl_loss             | 0.00755   |\n",
      "|    kl_mean_loss        | 0.00745   |\n",
      "|    kl_std_loss         | 0.000103  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 4057      |\n",
      "|    penalty_temperature | 2.08      |\n",
      "|    policy_loss         | 11.3      |\n",
      "|    policy_mean_loss    | 5.64      |\n",
      "|    policy_std_loss     | 5.65      |\n",
      "|    std                 | 0.491     |\n",
      "|    temperature         | 0.815     |\n",
      "|    temperature_loss    | -3.14     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 284       |\n",
      "|    ep_rew_mean         | -105      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 16        |\n",
      "|    fps                 | 74        |\n",
      "|    time_elapsed        | 61        |\n",
      "|    total_timesteps     | 4539      |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 7.96      |\n",
      "|    alpha_mean          | 0.629     |\n",
      "|    alpha_mean_loss     | 0.0165    |\n",
      "|    alpha_std           | 10.5      |\n",
      "|    alpha_std_loss      | -4.69e-05 |\n",
      "|    critic_loss         | 2.85      |\n",
      "|    dual_loss           | -3.33     |\n",
      "|    kl_loss             | 0.00873   |\n",
      "|    kl_mean_loss        | 0.00864   |\n",
      "|    kl_std_loss         | 8.9e-05   |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 4332      |\n",
      "|    penalty_temperature | 2.16      |\n",
      "|    policy_loss         | 11.3      |\n",
      "|    policy_mean_loss    | 5.63      |\n",
      "|    policy_std_loss     | 5.65      |\n",
      "|    std                 | 0.491     |\n",
      "|    temperature         | 0.819     |\n",
      "|    temperature_loss    | -3.35     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 398      |\n",
      "|    ep_rew_mean         | -102     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 20       |\n",
      "|    fps                 | 72       |\n",
      "|    time_elapsed        | 109      |\n",
      "|    total_timesteps     | 7957     |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 7.91     |\n",
      "|    alpha_mean          | 0.382    |\n",
      "|    alpha_mean_loss     | 0.0112   |\n",
      "|    alpha_std           | 10.7     |\n",
      "|    alpha_std_loss      | -4.9e-05 |\n",
      "|    critic_loss         | 0.476    |\n",
      "|    dual_loss           | -3.06    |\n",
      "|    kl_loss             | 0.0042   |\n",
      "|    kl_mean_loss        | 0.00411  |\n",
      "|    kl_std_loss         | 9.18e-05 |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 7716     |\n",
      "|    penalty_temperature | 2.6      |\n",
      "|    policy_loss         | 11       |\n",
      "|    policy_mean_loss    | 5.49     |\n",
      "|    policy_std_loss     | 5.48     |\n",
      "|    std                 | 0.481    |\n",
      "|    temperature         | 0.591    |\n",
      "|    temperature_loss    | -3.07    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 471       |\n",
      "|    ep_rew_mean         | -99.5     |\n",
      "| time/                  |           |\n",
      "|    episodes            | 24        |\n",
      "|    fps                 | 83        |\n",
      "|    time_elapsed        | 135       |\n",
      "|    total_timesteps     | 11299     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 8.12      |\n",
      "|    alpha_mean          | 0.264     |\n",
      "|    alpha_mean_loss     | 0.00672   |\n",
      "|    alpha_std           | 10.8      |\n",
      "|    alpha_std_loss      | -0.000134 |\n",
      "|    critic_loss         | 0.432     |\n",
      "|    dual_loss           | -2.58     |\n",
      "|    kl_loss             | 0.00399   |\n",
      "|    kl_mean_loss        | 0.00382   |\n",
      "|    kl_std_loss         | 0.000177  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 9558      |\n",
      "|    penalty_temperature | 2.88      |\n",
      "|    policy_loss         | 10.7      |\n",
      "|    policy_mean_loss    | 5.35      |\n",
      "|    policy_std_loss     | 5.35      |\n",
      "|    std                 | 0.473     |\n",
      "|    temperature         | 0.391     |\n",
      "|    temperature_loss    | -2.59     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 413       |\n",
      "|    ep_rew_mean         | -99.7     |\n",
      "| time/                  |           |\n",
      "|    episodes            | 28        |\n",
      "|    fps                 | 72        |\n",
      "|    time_elapsed        | 160       |\n",
      "|    total_timesteps     | 11573     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 7.86      |\n",
      "|    alpha_mean          | 0.155     |\n",
      "|    alpha_mean_loss     | 0.0032    |\n",
      "|    alpha_std           | 11.6      |\n",
      "|    alpha_std_loss      | -0.000392 |\n",
      "|    critic_loss         | 3.02      |\n",
      "|    dual_loss           | -2.27     |\n",
      "|    kl_loss             | 0.00345   |\n",
      "|    kl_mean_loss        | 0.00302   |\n",
      "|    kl_std_loss         | 0.000438  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 11372     |\n",
      "|    penalty_temperature | 3.22      |\n",
      "|    policy_loss         | 10.1      |\n",
      "|    policy_mean_loss    | 5.07      |\n",
      "|    policy_std_loss     | 5.06      |\n",
      "|    std                 | 0.464     |\n",
      "|    temperature         | 0.243     |\n",
      "|    temperature_loss    | -2.27     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 370       |\n",
      "|    ep_rew_mean         | -100      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 32        |\n",
      "|    fps                 | 72        |\n",
      "|    time_elapsed        | 163       |\n",
      "|    total_timesteps     | 11841     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 7.84      |\n",
      "|    alpha_mean          | 0.15      |\n",
      "|    alpha_mean_loss     | 0.00151   |\n",
      "|    alpha_std           | 11.7      |\n",
      "|    alpha_std_loss      | -0.000398 |\n",
      "|    critic_loss         | 2.34      |\n",
      "|    dual_loss           | -2.22     |\n",
      "|    kl_loss             | 0.00495   |\n",
      "|    kl_mean_loss        | 0.0045    |\n",
      "|    kl_std_loss         | 0.000445  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 11624     |\n",
      "|    penalty_temperature | 3.28      |\n",
      "|    policy_loss         | 10        |\n",
      "|    policy_mean_loss    | 5.02      |\n",
      "|    policy_std_loss     | 5.03      |\n",
      "|    std                 | 0.462     |\n",
      "|    temperature         | 0.252     |\n",
      "|    temperature_loss    | -2.22     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 336       |\n",
      "|    ep_rew_mean         | -100      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 36        |\n",
      "|    fps                 | 72        |\n",
      "|    time_elapsed        | 167       |\n",
      "|    total_timesteps     | 12098     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 7.95      |\n",
      "|    alpha_mean          | 0.149     |\n",
      "|    alpha_mean_loss     | 0.000433  |\n",
      "|    alpha_std           | 11.8      |\n",
      "|    alpha_std_loss      | -0.000248 |\n",
      "|    critic_loss         | 0.837     |\n",
      "|    dual_loss           | -2.13     |\n",
      "|    kl_loss             | 0.00583   |\n",
      "|    kl_mean_loss        | 0.00554   |\n",
      "|    kl_std_loss         | 0.000295  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 11897     |\n",
      "|    penalty_temperature | 3.33      |\n",
      "|    policy_loss         | 10.1      |\n",
      "|    policy_mean_loss    | 5.03      |\n",
      "|    policy_std_loss     | 5.04      |\n",
      "|    std                 | 0.461     |\n",
      "|    temperature         | 0.282     |\n",
      "|    temperature_loss    | -2.13     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 309       |\n",
      "|    ep_rew_mean         | -101      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 40        |\n",
      "|    fps                 | 72        |\n",
      "|    time_elapsed        | 171       |\n",
      "|    total_timesteps     | 12379     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 7.97      |\n",
      "|    alpha_mean          | 0.15      |\n",
      "|    alpha_mean_loss     | 0.000311  |\n",
      "|    alpha_std           | 11.8      |\n",
      "|    alpha_std_loss      | -0.000265 |\n",
      "|    critic_loss         | 1.14      |\n",
      "|    dual_loss           | -2.06     |\n",
      "|    kl_loss             | 0.00598   |\n",
      "|    kl_mean_loss        | 0.00567   |\n",
      "|    kl_std_loss         | 0.000312  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 12170     |\n",
      "|    penalty_temperature | 3.41      |\n",
      "|    policy_loss         | 10        |\n",
      "|    policy_mean_loss    | 5.01      |\n",
      "|    policy_std_loss     | 5.01      |\n",
      "|    std                 | 0.46      |\n",
      "|    temperature         | 0.311     |\n",
      "|    temperature_loss    | -2.06     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 289       |\n",
      "|    ep_rew_mean         | -102      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 44        |\n",
      "|    fps                 | 72        |\n",
      "|    time_elapsed        | 175       |\n",
      "|    total_timesteps     | 12696     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 7.81      |\n",
      "|    alpha_mean          | 0.151     |\n",
      "|    alpha_mean_loss     | -4.47e-05 |\n",
      "|    alpha_std           | 11.9      |\n",
      "|    alpha_std_loss      | -0.000347 |\n",
      "|    critic_loss         | 1.95      |\n",
      "|    dual_loss           | -2.12     |\n",
      "|    kl_loss             | 0.00646   |\n",
      "|    kl_mean_loss        | 0.00607   |\n",
      "|    kl_std_loss         | 0.000395  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 12474     |\n",
      "|    penalty_temperature | 3.52      |\n",
      "|    policy_loss         | 9.92      |\n",
      "|    policy_mean_loss    | 4.97      |\n",
      "|    policy_std_loss     | 4.95      |\n",
      "|    std                 | 0.457     |\n",
      "|    temperature         | 0.331     |\n",
      "|    temperature_loss    | -2.12     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 272       |\n",
      "|    ep_rew_mean         | -102      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 48        |\n",
      "|    fps                 | 72        |\n",
      "|    time_elapsed        | 180       |\n",
      "|    total_timesteps     | 13038     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 7.65      |\n",
      "|    alpha_mean          | 0.154     |\n",
      "|    alpha_mean_loss     | -0.000331 |\n",
      "|    alpha_std           | 12.1      |\n",
      "|    alpha_std_loss      | -0.000598 |\n",
      "|    critic_loss         | 2.88      |\n",
      "|    dual_loss           | -2.1      |\n",
      "|    kl_loss             | 0.00712   |\n",
      "|    kl_mean_loss        | 0.00647   |\n",
      "|    kl_std_loss         | 0.000646  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 12823     |\n",
      "|    penalty_temperature | 3.63      |\n",
      "|    policy_loss         | 9.74      |\n",
      "|    policy_mean_loss    | 4.88      |\n",
      "|    policy_std_loss     | 4.86      |\n",
      "|    std                 | 0.454     |\n",
      "|    temperature         | 0.358     |\n",
      "|    temperature_loss    | -2.1      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 258       |\n",
      "|    ep_rew_mean         | -103      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 52        |\n",
      "|    fps                 | 72        |\n",
      "|    time_elapsed        | 186       |\n",
      "|    total_timesteps     | 13420     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 7.5       |\n",
      "|    alpha_mean          | 0.157     |\n",
      "|    alpha_mean_loss     | -0.00146  |\n",
      "|    alpha_std           | 12.2      |\n",
      "|    alpha_std_loss      | -0.000453 |\n",
      "|    critic_loss         | 1.88      |\n",
      "|    dual_loss           | -2.22     |\n",
      "|    kl_loss             | 0.00824   |\n",
      "|    kl_mean_loss        | 0.00774   |\n",
      "|    kl_std_loss         | 0.000502  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 13206     |\n",
      "|    penalty_temperature | 3.7       |\n",
      "|    policy_loss         | 9.71      |\n",
      "|    policy_mean_loss    | 4.87      |\n",
      "|    policy_std_loss     | 4.85      |\n",
      "|    std                 | 0.452     |\n",
      "|    temperature         | 0.39      |\n",
      "|    temperature_loss    | -2.22     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 248       |\n",
      "|    ep_rew_mean         | -104      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 56        |\n",
      "|    fps                 | 72        |\n",
      "|    time_elapsed        | 191       |\n",
      "|    total_timesteps     | 13867     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 7.4       |\n",
      "|    alpha_mean          | 0.166     |\n",
      "|    alpha_mean_loss     | -0.00202  |\n",
      "|    alpha_std           | 12.3      |\n",
      "|    alpha_std_loss      | -0.000289 |\n",
      "|    critic_loss         | 2.71      |\n",
      "|    dual_loss           | -2.31     |\n",
      "|    kl_loss             | 0.00901   |\n",
      "|    kl_mean_loss        | 0.00868   |\n",
      "|    kl_std_loss         | 0.000338  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 13584     |\n",
      "|    penalty_temperature | 3.75      |\n",
      "|    policy_loss         | 9.7       |\n",
      "|    policy_mean_loss    | 4.86      |\n",
      "|    policy_std_loss     | 4.83      |\n",
      "|    std                 | 0.45      |\n",
      "|    temperature         | 0.437     |\n",
      "|    temperature_loss    | -2.31     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 262       |\n",
      "|    ep_rew_mean         | -103      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 60        |\n",
      "|    fps                 | 79        |\n",
      "|    time_elapsed        | 197       |\n",
      "|    total_timesteps     | 15730     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 7.16      |\n",
      "|    alpha_mean          | 0.178     |\n",
      "|    alpha_mean_loss     | -0.00263  |\n",
      "|    alpha_std           | 12.4      |\n",
      "|    alpha_std_loss      | -0.000241 |\n",
      "|    critic_loss         | 4.77      |\n",
      "|    dual_loss           | -2.49     |\n",
      "|    kl_loss             | 0.01      |\n",
      "|    kl_mean_loss        | 0.00974   |\n",
      "|    kl_std_loss         | 0.00029   |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 13989     |\n",
      "|    penalty_temperature | 3.78      |\n",
      "|    policy_loss         | 9.64      |\n",
      "|    policy_mean_loss    | 4.84      |\n",
      "|    policy_std_loss     | 4.8       |\n",
      "|    std                 | 0.447     |\n",
      "|    temperature         | 0.485     |\n",
      "|    temperature_loss    | -2.49     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 322       |\n",
      "|    ep_rew_mean         | -100      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 64        |\n",
      "|    fps                 | 77        |\n",
      "|    time_elapsed        | 266       |\n",
      "|    total_timesteps     | 20628     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 7.94      |\n",
      "|    alpha_mean          | 0.315     |\n",
      "|    alpha_mean_loss     | -0.0202   |\n",
      "|    alpha_std           | 12.6      |\n",
      "|    alpha_std_loss      | -1.26e-05 |\n",
      "|    critic_loss         | 2.84      |\n",
      "|    dual_loss           | -1.88     |\n",
      "|    kl_loss             | 0.0329    |\n",
      "|    kl_mean_loss        | 0.0328    |\n",
      "|    kl_std_loss         | 6.28e-05  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 18887     |\n",
      "|    penalty_temperature | 4.01      |\n",
      "|    policy_loss         | 9.79      |\n",
      "|    policy_mean_loss    | 4.94      |\n",
      "|    policy_std_loss     | 4.85      |\n",
      "|    std                 | 0.444     |\n",
      "|    temperature         | 0.733     |\n",
      "|    temperature_loss    | -1.86     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 309       |\n",
      "|    ep_rew_mean         | -101      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 68        |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 293       |\n",
      "|    total_timesteps     | 20998     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 14.6      |\n",
      "|    alpha_mean          | 0.511     |\n",
      "|    alpha_mean_loss     | -0.0366   |\n",
      "|    alpha_std           | 12.8      |\n",
      "|    alpha_std_loss      | -0.000243 |\n",
      "|    critic_loss         | 6.71      |\n",
      "|    dual_loss           | 4.26      |\n",
      "|    kl_loss             | 0.0573    |\n",
      "|    kl_mean_loss        | 0.057     |\n",
      "|    kl_std_loss         | 0.000295  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 20782     |\n",
      "|    penalty_temperature | 4.79      |\n",
      "|    policy_loss         | 10.3      |\n",
      "|    policy_mean_loss    | 5.17      |\n",
      "|    policy_std_loss     | 5.1       |\n",
      "|    std                 | 0.452     |\n",
      "|    temperature         | 1.4       |\n",
      "|    temperature_loss    | 4.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 296       |\n",
      "|    ep_rew_mean         | -102      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 72        |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 297       |\n",
      "|    total_timesteps     | 21304     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 15.7      |\n",
      "|    alpha_mean          | 0.542     |\n",
      "|    alpha_mean_loss     | -0.0424   |\n",
      "|    alpha_std           | 12.8      |\n",
      "|    alpha_std_loss      | -8.45e-05 |\n",
      "|    critic_loss         | 8.52      |\n",
      "|    dual_loss           | 5.44      |\n",
      "|    kl_loss             | 0.0642    |\n",
      "|    kl_mean_loss        | 0.0641    |\n",
      "|    kl_std_loss         | 0.000136  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 21072     |\n",
      "|    penalty_temperature | 4.89      |\n",
      "|    policy_loss         | 10.2      |\n",
      "|    policy_mean_loss    | 5.16      |\n",
      "|    policy_std_loss     | 5.07      |\n",
      "|    std                 | 0.453     |\n",
      "|    temperature         | 1.47      |\n",
      "|    temperature_loss    | 5.49      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 284       |\n",
      "|    ep_rew_mean         | -102      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 76        |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 302       |\n",
      "|    total_timesteps     | 21591     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 16.1      |\n",
      "|    alpha_mean          | 0.574     |\n",
      "|    alpha_mean_loss     | -0.0393   |\n",
      "|    alpha_std           | 12.9      |\n",
      "|    alpha_std_loss      | -7.14e-05 |\n",
      "|    critic_loss         | 4.16      |\n",
      "|    dual_loss           | 5.75      |\n",
      "|    kl_loss             | 0.0623    |\n",
      "|    kl_mean_loss        | 0.0622    |\n",
      "|    kl_std_loss         | 0.000123  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 21361     |\n",
      "|    penalty_temperature | 4.98      |\n",
      "|    policy_loss         | 10.2      |\n",
      "|    policy_mean_loss    | 5.16      |\n",
      "|    policy_std_loss     | 5.08      |\n",
      "|    std                 | 0.454     |\n",
      "|    temperature         | 1.52      |\n",
      "|    temperature_loss    | 5.79      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 276       |\n",
      "|    ep_rew_mean         | -103      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 80        |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 309       |\n",
      "|    total_timesteps     | 22078     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 17        |\n",
      "|    alpha_mean          | 0.632     |\n",
      "|    alpha_mean_loss     | -0.0424   |\n",
      "|    alpha_std           | 12.9      |\n",
      "|    alpha_std_loss      | -2.33e-05 |\n",
      "|    critic_loss         | 14.4      |\n",
      "|    dual_loss           | 6.7       |\n",
      "|    kl_loss             | 0.0678    |\n",
      "|    kl_mean_loss        | 0.0677    |\n",
      "|    kl_std_loss         | 7.47e-05  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 21854     |\n",
      "|    penalty_temperature | 5.13      |\n",
      "|    policy_loss         | 10.3      |\n",
      "|    policy_mean_loss    | 5.18      |\n",
      "|    policy_std_loss     | 5.09      |\n",
      "|    std                 | 0.455     |\n",
      "|    temperature         | 1.62      |\n",
      "|    temperature_loss    | 6.74      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 267      |\n",
      "|    ep_rew_mean         | -104     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 84       |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 313      |\n",
      "|    total_timesteps     | 22405    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 18.1     |\n",
      "|    alpha_mean          | 0.676    |\n",
      "|    alpha_mean_loss     | -0.0515  |\n",
      "|    alpha_std           | 12.8     |\n",
      "|    alpha_std_loss      | -3.1e-05 |\n",
      "|    critic_loss         | 15.8     |\n",
      "|    dual_loss           | 7.74     |\n",
      "|    kl_loss             | 0.0786   |\n",
      "|    kl_mean_loss        | 0.0786   |\n",
      "|    kl_std_loss         | 8.23e-05 |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 22182    |\n",
      "|    penalty_temperature | 5.23     |\n",
      "|    policy_loss         | 10.3     |\n",
      "|    policy_mean_loss    | 5.21     |\n",
      "|    policy_std_loss     | 5.1      |\n",
      "|    std                 | 0.455    |\n",
      "|    temperature         | 1.7      |\n",
      "|    temperature_loss    | 7.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 259       |\n",
      "|    ep_rew_mean         | -104      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 88        |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 319       |\n",
      "|    total_timesteps     | 22781     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 18.9      |\n",
      "|    alpha_mean          | 0.731     |\n",
      "|    alpha_mean_loss     | -0.0596   |\n",
      "|    alpha_std           | 12.8      |\n",
      "|    alpha_std_loss      | -5.62e-05 |\n",
      "|    critic_loss         | 6.6       |\n",
      "|    dual_loss           | 8.49      |\n",
      "|    kl_loss             | 0.0889    |\n",
      "|    kl_mean_loss        | 0.0888    |\n",
      "|    kl_std_loss         | 0.000107  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 22574     |\n",
      "|    penalty_temperature | 5.36      |\n",
      "|    policy_loss         | 10.4      |\n",
      "|    policy_mean_loss    | 5.24      |\n",
      "|    policy_std_loss     | 5.13      |\n",
      "|    std                 | 0.457     |\n",
      "|    temperature         | 1.81      |\n",
      "|    temperature_loss    | 8.55      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 251       |\n",
      "|    ep_rew_mean         | -105      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 92        |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 324       |\n",
      "|    total_timesteps     | 23131     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 18.9      |\n",
      "|    alpha_mean          | 0.787     |\n",
      "|    alpha_mean_loss     | -0.0643   |\n",
      "|    alpha_std           | 12.8      |\n",
      "|    alpha_std_loss      | -3.63e-05 |\n",
      "|    critic_loss         | 11.7      |\n",
      "|    dual_loss           | 8.39      |\n",
      "|    kl_loss             | 0.0958    |\n",
      "|    kl_mean_loss        | 0.0957    |\n",
      "|    kl_std_loss         | 8.75e-05  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 22945     |\n",
      "|    penalty_temperature | 5.46      |\n",
      "|    policy_loss         | 10.4      |\n",
      "|    policy_mean_loss    | 5.25      |\n",
      "|    policy_std_loss     | 5.14      |\n",
      "|    std                 | 0.457     |\n",
      "|    temperature         | 1.91      |\n",
      "|    temperature_loss    | 8.46      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 244       |\n",
      "|    ep_rew_mean         | -105      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 96        |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 328       |\n",
      "|    total_timesteps     | 23392     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 19.2      |\n",
      "|    alpha_mean          | 0.827     |\n",
      "|    alpha_mean_loss     | -0.0647   |\n",
      "|    alpha_std           | 12.8      |\n",
      "|    alpha_std_loss      | -5.15e-05 |\n",
      "|    critic_loss         | 8.81      |\n",
      "|    dual_loss           | 8.69      |\n",
      "|    kl_loss             | 0.0978    |\n",
      "|    kl_mean_loss        | 0.0977    |\n",
      "|    kl_std_loss         | 0.000103  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 23193     |\n",
      "|    penalty_temperature | 5.54      |\n",
      "|    policy_loss         | 10.4      |\n",
      "|    policy_mean_loss    | 5.27      |\n",
      "|    policy_std_loss     | 5.15      |\n",
      "|    std                 | 0.458     |\n",
      "|    temperature         | 1.99      |\n",
      "|    temperature_loss    | 8.75      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 237       |\n",
      "|    ep_rew_mean         | -106      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 100       |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 332       |\n",
      "|    total_timesteps     | 23666     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 19.7      |\n",
      "|    alpha_mean          | 0.877     |\n",
      "|    alpha_mean_loss     | -0.076    |\n",
      "|    alpha_std           | 12.8      |\n",
      "|    alpha_std_loss      | -6.82e-05 |\n",
      "|    critic_loss         | 12.6      |\n",
      "|    dual_loss           | 9.17      |\n",
      "|    kl_loss             | 0.111     |\n",
      "|    kl_mean_loss        | 0.111     |\n",
      "|    kl_std_loss         | 0.000119  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 23482     |\n",
      "|    penalty_temperature | 5.63      |\n",
      "|    policy_loss         | 10.5      |\n",
      "|    policy_mean_loss    | 5.29      |\n",
      "|    policy_std_loss     | 5.17      |\n",
      "|    std                 | 0.459     |\n",
      "|    temperature         | 2.08      |\n",
      "|    temperature_loss    | 9.25      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 220       |\n",
      "|    ep_rew_mean         | -107      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 104       |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 334       |\n",
      "|    total_timesteps     | 23833     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 20.1      |\n",
      "|    alpha_mean          | 0.908     |\n",
      "|    alpha_mean_loss     | -0.082    |\n",
      "|    alpha_std           | 12.8      |\n",
      "|    alpha_std_loss      | -4.58e-05 |\n",
      "|    critic_loss         | 14.2      |\n",
      "|    dual_loss           | 9.5       |\n",
      "|    kl_loss             | 0.118     |\n",
      "|    kl_mean_loss        | 0.118     |\n",
      "|    kl_std_loss         | 9.71e-05  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 23650     |\n",
      "|    penalty_temperature | 5.68      |\n",
      "|    policy_loss         | 10.5      |\n",
      "|    policy_mean_loss    | 5.31      |\n",
      "|    policy_std_loss     | 5.18      |\n",
      "|    std                 | 0.46      |\n",
      "|    temperature         | 2.14      |\n",
      "|    temperature_loss    | 9.58      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 203       |\n",
      "|    ep_rew_mean         | -107      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 108       |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 337       |\n",
      "|    total_timesteps     | 24040     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 19.9      |\n",
      "|    alpha_mean          | 0.943     |\n",
      "|    alpha_mean_loss     | -0.121    |\n",
      "|    alpha_std           | 12.8      |\n",
      "|    alpha_std_loss      | -0.000121 |\n",
      "|    critic_loss         | 19.8      |\n",
      "|    dual_loss           | 9.17      |\n",
      "|    kl_loss             | 0.159     |\n",
      "|    kl_mean_loss        | 0.159     |\n",
      "|    kl_std_loss         | 0.000173  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 23832     |\n",
      "|    penalty_temperature | 5.73      |\n",
      "|    policy_loss         | 10.6      |\n",
      "|    policy_mean_loss    | 5.39      |\n",
      "|    policy_std_loss     | 5.21      |\n",
      "|    std                 | 0.46      |\n",
      "|    temperature         | 2.2       |\n",
      "|    temperature_loss    | 9.29      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 200       |\n",
      "|    ep_rew_mean         | -108      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 112       |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 341       |\n",
      "|    total_timesteps     | 24293     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 19.1      |\n",
      "|    alpha_mean          | 1         |\n",
      "|    alpha_mean_loss     | -0.11     |\n",
      "|    alpha_std           | 12.9      |\n",
      "|    alpha_std_loss      | -5.52e-05 |\n",
      "|    critic_loss         | 17.8      |\n",
      "|    dual_loss           | 8.42      |\n",
      "|    kl_loss             | 0.151     |\n",
      "|    kl_mean_loss        | 0.15      |\n",
      "|    kl_std_loss         | 0.000107  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 24116     |\n",
      "|    penalty_temperature | 5.82      |\n",
      "|    policy_loss         | 10.6      |\n",
      "|    policy_mean_loss    | 5.38      |\n",
      "|    policy_std_loss     | 5.2       |\n",
      "|    std                 | 0.461     |\n",
      "|    temperature         | 2.29      |\n",
      "|    temperature_loss    | 8.53      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 199       |\n",
      "|    ep_rew_mean         | -108      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 116       |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 343       |\n",
      "|    total_timesteps     | 24454     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 19.1      |\n",
      "|    alpha_mean          | 1.04      |\n",
      "|    alpha_mean_loss     | -0.152    |\n",
      "|    alpha_std           | 12.9      |\n",
      "|    alpha_std_loss      | -6.73e-05 |\n",
      "|    critic_loss         | 20.6      |\n",
      "|    dual_loss           | 8.24      |\n",
      "|    kl_loss             | 0.194     |\n",
      "|    kl_mean_loss        | 0.194     |\n",
      "|    kl_std_loss         | 0.000119  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 24278     |\n",
      "|    penalty_temperature | 5.86      |\n",
      "|    policy_loss         | 10.7      |\n",
      "|    policy_mean_loss    | 5.44      |\n",
      "|    policy_std_loss     | 5.22      |\n",
      "|    std                 | 0.461     |\n",
      "|    temperature         | 2.34      |\n",
      "|    temperature_loss    | 8.39      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 167       |\n",
      "|    ep_rew_mean         | -109      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 120       |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 346       |\n",
      "|    total_timesteps     | 24650     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 19.3      |\n",
      "|    alpha_mean          | 1.07      |\n",
      "|    alpha_mean_loss     | -0.148    |\n",
      "|    alpha_std           | 12.9      |\n",
      "|    alpha_std_loss      | -5.15e-05 |\n",
      "|    critic_loss         | 29        |\n",
      "|    dual_loss           | 8.41      |\n",
      "|    kl_loss             | 0.191     |\n",
      "|    kl_mean_loss        | 0.191     |\n",
      "|    kl_std_loss         | 0.000103  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 24444     |\n",
      "|    penalty_temperature | 5.91      |\n",
      "|    policy_loss         | 10.7      |\n",
      "|    policy_mean_loss    | 5.45      |\n",
      "|    policy_std_loss     | 5.23      |\n",
      "|    std                 | 0.462     |\n",
      "|    temperature         | 2.39      |\n",
      "|    temperature_loss    | 8.56      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 136       |\n",
      "|    ep_rew_mean         | -110      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 124       |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 349       |\n",
      "|    total_timesteps     | 24853     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 18.1      |\n",
      "|    alpha_mean          | 1.13      |\n",
      "|    alpha_mean_loss     | -0.195    |\n",
      "|    alpha_std           | 13        |\n",
      "|    alpha_std_loss      | -3.65e-05 |\n",
      "|    critic_loss         | 27        |\n",
      "|    dual_loss           | 7.08      |\n",
      "|    kl_loss             | 0.24      |\n",
      "|    kl_mean_loss        | 0.24      |\n",
      "|    kl_std_loss         | 8.84e-05  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 24673     |\n",
      "|    penalty_temperature | 5.98      |\n",
      "|    policy_loss         | 10.7      |\n",
      "|    policy_mean_loss    | 5.5       |\n",
      "|    policy_std_loss     | 5.24      |\n",
      "|    std                 | 0.463     |\n",
      "|    temperature         | 2.46      |\n",
      "|    temperature_loss    | 7.28      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 134      |\n",
      "|    ep_rew_mean         | -111     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 128      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 351      |\n",
      "|    total_timesteps     | 25011    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 18.5     |\n",
      "|    alpha_mean          | 1.16     |\n",
      "|    alpha_mean_loss     | -0.216   |\n",
      "|    alpha_std           | 13       |\n",
      "|    alpha_std_loss      | -3.7e-05 |\n",
      "|    critic_loss         | 38.1     |\n",
      "|    dual_loss           | 7.42     |\n",
      "|    kl_loss             | 0.263    |\n",
      "|    kl_mean_loss        | 0.263    |\n",
      "|    kl_std_loss         | 8.88e-05 |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 24825    |\n",
      "|    penalty_temperature | 6.03     |\n",
      "|    policy_loss         | 10.8     |\n",
      "|    policy_mean_loss    | 5.54     |\n",
      "|    policy_std_loss     | 5.25     |\n",
      "|    std                 | 0.463    |\n",
      "|    temperature         | 2.51     |\n",
      "|    temperature_loss    | 7.63     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 134       |\n",
      "|    ep_rew_mean         | -112      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 132       |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 354       |\n",
      "|    total_timesteps     | 25201     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 17.4      |\n",
      "|    alpha_mean          | 1.21      |\n",
      "|    alpha_mean_loss     | -0.204    |\n",
      "|    alpha_std           | 13        |\n",
      "|    alpha_std_loss      | -0.000122 |\n",
      "|    critic_loss         | 41.5      |\n",
      "|    dual_loss           | 6.33      |\n",
      "|    kl_loss             | 0.252     |\n",
      "|    kl_mean_loss        | 0.252     |\n",
      "|    kl_std_loss         | 0.000174  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 25015     |\n",
      "|    penalty_temperature | 6.08      |\n",
      "|    policy_loss         | 10.8      |\n",
      "|    policy_mean_loss    | 5.53      |\n",
      "|    policy_std_loss     | 5.28      |\n",
      "|    std                 | 0.464     |\n",
      "|    temperature         | 2.57      |\n",
      "|    temperature_loss    | 6.53      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 133       |\n",
      "|    ep_rew_mean         | -112      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 136       |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 356       |\n",
      "|    total_timesteps     | 25392     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 17.2      |\n",
      "|    alpha_mean          | 1.24      |\n",
      "|    alpha_mean_loss     | -0.244    |\n",
      "|    alpha_std           | 13.1      |\n",
      "|    alpha_std_loss      | -9.71e-05 |\n",
      "|    critic_loss         | 38.7      |\n",
      "|    dual_loss           | 6         |\n",
      "|    kl_loss             | 0.294     |\n",
      "|    kl_mean_loss        | 0.293     |\n",
      "|    kl_std_loss         | 0.000149  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 25160     |\n",
      "|    penalty_temperature | 6.13      |\n",
      "|    policy_loss         | 10.9      |\n",
      "|    policy_mean_loss    | 5.6       |\n",
      "|    policy_std_loss     | 5.3       |\n",
      "|    std                 | 0.465     |\n",
      "|    temperature         | 2.62      |\n",
      "|    temperature_loss    | 6.24      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 132       |\n",
      "|    ep_rew_mean         | -112      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 140       |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 359       |\n",
      "|    total_timesteps     | 25540     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 18.1      |\n",
      "|    alpha_mean          | 1.29      |\n",
      "|    alpha_mean_loss     | -0.301    |\n",
      "|    alpha_std           | 13.1      |\n",
      "|    alpha_std_loss      | -0.000217 |\n",
      "|    critic_loss         | 146       |\n",
      "|    dual_loss           | 6.73      |\n",
      "|    kl_loss             | 0.353     |\n",
      "|    kl_mean_loss        | 0.353     |\n",
      "|    kl_std_loss         | 0.000269  |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 25370     |\n",
      "|    penalty_temperature | 6.19      |\n",
      "|    policy_loss         | 11        |\n",
      "|    policy_mean_loss    | 5.7       |\n",
      "|    policy_std_loss     | 5.34      |\n",
      "|    std                 | 0.466     |\n",
      "|    temperature         | 2.7       |\n",
      "|    temperature_loss    | 7.03      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 130       |\n",
      "|    ep_rew_mean         | -112      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 144       |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 361       |\n",
      "|    total_timesteps     | 25741     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 19.8      |\n",
      "|    alpha_mean          | 1.34      |\n",
      "|    alpha_mean_loss     | -0.333    |\n",
      "|    alpha_std           | 13.2      |\n",
      "|    alpha_std_loss      | -0.000447 |\n",
      "|    critic_loss         | 110       |\n",
      "|    dual_loss           | 8.21      |\n",
      "|    kl_loss             | 0.388     |\n",
      "|    kl_mean_loss        | 0.387     |\n",
      "|    kl_std_loss         | 0.0005    |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 25558     |\n",
      "|    penalty_temperature | 6.24      |\n",
      "|    policy_loss         | 11.2      |\n",
      "|    policy_mean_loss    | 5.77      |\n",
      "|    policy_std_loss     | 5.4       |\n",
      "|    std                 | 0.467     |\n",
      "|    temperature         | 2.78      |\n",
      "|    temperature_loss    | 8.55      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/               |           |\n",
      "|    ep_len_mean         | 129       |\n",
      "|    ep_rew_mean         | -113      |\n",
      "| time/                  |           |\n",
      "|    episodes            | 148       |\n",
      "|    fps                 | 71        |\n",
      "|    time_elapsed        | 364       |\n",
      "|    total_timesteps     | 25936     |\n",
      "| train/                 |           |\n",
      "|    actor_loss          | 23.7      |\n",
      "|    alpha_mean          | 1.39      |\n",
      "|    alpha_mean_loss     | -0.439    |\n",
      "|    alpha_std           | 13.4      |\n",
      "|    alpha_std_loss      | -0.000988 |\n",
      "|    critic_loss         | 512       |\n",
      "|    dual_loss           | 11.8      |\n",
      "|    kl_loss             | 0.495     |\n",
      "|    kl_mean_loss        | 0.494     |\n",
      "|    kl_std_loss         | 0.00104   |\n",
      "|    learning_rate       | 0.0003    |\n",
      "|    n_updates           | 25751     |\n",
      "|    penalty_temperature | 6.3       |\n",
      "|    policy_loss         | 11.5      |\n",
      "|    policy_mean_loss    | 5.95      |\n",
      "|    policy_std_loss     | 5.5       |\n",
      "|    std                 | 0.469     |\n",
      "|    temperature         | 2.88      |\n",
      "|    temperature_loss    | 12.2      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 127      |\n",
      "|    ep_rew_mean         | -113     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 152      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 367      |\n",
      "|    total_timesteps     | 26108    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 29.3     |\n",
      "|    alpha_mean          | 1.44     |\n",
      "|    alpha_mean_loss     | -0.456   |\n",
      "|    alpha_std           | 13.5     |\n",
      "|    alpha_std_loss      | -0.00149 |\n",
      "|    critic_loss         | 435      |\n",
      "|    dual_loss           | 17.2     |\n",
      "|    kl_loss             | 0.515    |\n",
      "|    kl_mean_loss        | 0.514    |\n",
      "|    kl_std_loss         | 0.00154  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 25921    |\n",
      "|    penalty_temperature | 6.35     |\n",
      "|    policy_loss         | 11.6     |\n",
      "|    policy_mean_loss    | 6.03     |\n",
      "|    policy_std_loss     | 5.59     |\n",
      "|    std                 | 0.471    |\n",
      "|    temperature         | 2.97     |\n",
      "|    temperature_loss    | 17.7     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 124      |\n",
      "|    ep_rew_mean         | -113     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 156      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 369      |\n",
      "|    total_timesteps     | 26286    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 38.2     |\n",
      "|    alpha_mean          | 1.49     |\n",
      "|    alpha_mean_loss     | -0.471   |\n",
      "|    alpha_std           | 13.6     |\n",
      "|    alpha_std_loss      | -0.00205 |\n",
      "|    critic_loss         | 658      |\n",
      "|    dual_loss           | 25.9     |\n",
      "|    kl_loss             | 0.533    |\n",
      "|    kl_mean_loss        | 0.531    |\n",
      "|    kl_std_loss         | 0.0021   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 26102    |\n",
      "|    penalty_temperature | 6.4      |\n",
      "|    policy_loss         | 11.8     |\n",
      "|    policy_mean_loss    | 6.11     |\n",
      "|    policy_std_loss     | 5.69     |\n",
      "|    std                 | 0.474    |\n",
      "|    temperature         | 3.07     |\n",
      "|    temperature_loss    | 26.3     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 107      |\n",
      "|    ep_rew_mean         | -114     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 160      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 372      |\n",
      "|    total_timesteps     | 26478    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 48.1     |\n",
      "|    alpha_mean          | 1.54     |\n",
      "|    alpha_mean_loss     | -0.536   |\n",
      "|    alpha_std           | 13.7     |\n",
      "|    alpha_std_loss      | -0.0028  |\n",
      "|    critic_loss         | 1.55e+03 |\n",
      "|    dual_loss           | 35.5     |\n",
      "|    kl_loss             | 0.6      |\n",
      "|    kl_mean_loss        | 0.598    |\n",
      "|    kl_std_loss         | 0.00286  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 26298    |\n",
      "|    penalty_temperature | 6.45     |\n",
      "|    policy_loss         | 12       |\n",
      "|    policy_mean_loss    | 6.24     |\n",
      "|    policy_std_loss     | 5.79     |\n",
      "|    std                 | 0.478    |\n",
      "|    temperature         | 3.16     |\n",
      "|    temperature_loss    | 36       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 60.4     |\n",
      "|    ep_rew_mean         | -115     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 164      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 374      |\n",
      "|    total_timesteps     | 26667    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 65.9     |\n",
      "|    alpha_mean          | 1.6      |\n",
      "|    alpha_mean_loss     | -0.693   |\n",
      "|    alpha_std           | 13.8     |\n",
      "|    alpha_std_loss      | -0.00337 |\n",
      "|    critic_loss         | 3.2e+03  |\n",
      "|    dual_loss           | 52.8     |\n",
      "|    kl_loss             | 0.76     |\n",
      "|    kl_mean_loss        | 0.757    |\n",
      "|    kl_std_loss         | 0.00343  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 26479    |\n",
      "|    penalty_temperature | 6.51     |\n",
      "|    policy_loss         | 12.3     |\n",
      "|    policy_mean_loss    | 6.46     |\n",
      "|    policy_std_loss     | 5.87     |\n",
      "|    std                 | 0.481    |\n",
      "|    temperature         | 3.26     |\n",
      "|    temperature_loss    | 53.5     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 58.7     |\n",
      "|    ep_rew_mean         | -115     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 168      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 377      |\n",
      "|    total_timesteps     | 26866    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 78.9     |\n",
      "|    alpha_mean          | 1.65     |\n",
      "|    alpha_mean_loss     | -0.764   |\n",
      "|    alpha_std           | 13.9     |\n",
      "|    alpha_std_loss      | -0.00347 |\n",
      "|    critic_loss         | 3.92e+03 |\n",
      "|    dual_loss           | 65.6     |\n",
      "|    kl_loss             | 0.834    |\n",
      "|    kl_mean_loss        | 0.83     |\n",
      "|    kl_std_loss         | 0.00353  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 26651    |\n",
      "|    penalty_temperature | 6.56     |\n",
      "|    policy_loss         | 12.5     |\n",
      "|    policy_mean_loss    | 6.57     |\n",
      "|    policy_std_loss     | 5.94     |\n",
      "|    std                 | 0.484    |\n",
      "|    temperature         | 3.34     |\n",
      "|    temperature_loss    | 66.3     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 57.7     |\n",
      "|    ep_rew_mean         | -115     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 172      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 380      |\n",
      "|    total_timesteps     | 27077    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 127      |\n",
      "|    alpha_mean          | 1.72     |\n",
      "|    alpha_mean_loss     | -0.783   |\n",
      "|    alpha_std           | 14       |\n",
      "|    alpha_std_loss      | -0.0045  |\n",
      "|    critic_loss         | 1.26e+04 |\n",
      "|    dual_loss           | 113      |\n",
      "|    kl_loss             | 0.857    |\n",
      "|    kl_mean_loss        | 0.852    |\n",
      "|    kl_std_loss         | 0.00456  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 26890    |\n",
      "|    penalty_temperature | 6.63     |\n",
      "|    policy_loss         | 12.7     |\n",
      "|    policy_mean_loss    | 6.67     |\n",
      "|    policy_std_loss     | 6.05     |\n",
      "|    std                 | 0.488    |\n",
      "|    temperature         | 3.44     |\n",
      "|    temperature_loss    | 114      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 57.4     |\n",
      "|    ep_rew_mean         | -115     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 176      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 384      |\n",
      "|    total_timesteps     | 27330    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 214      |\n",
      "|    alpha_mean          | 1.8      |\n",
      "|    alpha_mean_loss     | -0.896   |\n",
      "|    alpha_std           | 14.1     |\n",
      "|    alpha_std_loss      | -0.00479 |\n",
      "|    critic_loss         | 2.32e+04 |\n",
      "|    dual_loss           | 200      |\n",
      "|    kl_loss             | 0.973    |\n",
      "|    kl_mean_loss        | 0.968    |\n",
      "|    kl_std_loss         | 0.00484  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 27143    |\n",
      "|    penalty_temperature | 6.71     |\n",
      "|    policy_loss         | 12.9     |\n",
      "|    policy_mean_loss    | 6.81     |\n",
      "|    policy_std_loss     | 6.13     |\n",
      "|    std                 | 0.493    |\n",
      "|    temperature         | 3.55     |\n",
      "|    temperature_loss    | 201      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 54.8     |\n",
      "|    ep_rew_mean         | -115     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 180      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 387      |\n",
      "|    total_timesteps     | 27557    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 318      |\n",
      "|    alpha_mean          | 1.86     |\n",
      "|    alpha_mean_loss     | -0.947   |\n",
      "|    alpha_std           | 14.1     |\n",
      "|    alpha_std_loss      | -0.00433 |\n",
      "|    critic_loss         | 6.56e+04 |\n",
      "|    dual_loss           | 304      |\n",
      "|    kl_loss             | 1.03     |\n",
      "|    kl_mean_loss        | 1.02     |\n",
      "|    kl_std_loss         | 0.00438  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 27372    |\n",
      "|    penalty_temperature | 6.78     |\n",
      "|    policy_loss         | 13       |\n",
      "|    policy_mean_loss    | 6.88     |\n",
      "|    policy_std_loss     | 6.16     |\n",
      "|    std                 | 0.497    |\n",
      "|    temperature         | 3.65     |\n",
      "|    temperature_loss    | 305      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 53.8     |\n",
      "|    ep_rew_mean         | -115     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 184      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 390      |\n",
      "|    total_timesteps     | 27786    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 494      |\n",
      "|    alpha_mean          | 1.93     |\n",
      "|    alpha_mean_loss     | -1.08    |\n",
      "|    alpha_std           | 14.2     |\n",
      "|    alpha_std_loss      | -0.00485 |\n",
      "|    critic_loss         | 1.01e+05 |\n",
      "|    dual_loss           | 479      |\n",
      "|    kl_loss             | 1.16     |\n",
      "|    kl_mean_loss        | 1.16     |\n",
      "|    kl_std_loss         | 0.00491  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 27598    |\n",
      "|    penalty_temperature | 6.85     |\n",
      "|    policy_loss         | 13.3     |\n",
      "|    policy_mean_loss    | 7.06     |\n",
      "|    policy_std_loss     | 6.25     |\n",
      "|    std                 | 0.501    |\n",
      "|    temperature         | 3.74     |\n",
      "|    temperature_loss    | 480      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 52.8     |\n",
      "|    ep_rew_mean         | -115     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 188      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 394      |\n",
      "|    total_timesteps     | 28056    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 710      |\n",
      "|    alpha_mean          | 2.01     |\n",
      "|    alpha_mean_loss     | -1.18    |\n",
      "|    alpha_std           | 14.3     |\n",
      "|    alpha_std_loss      | -0.00624 |\n",
      "|    critic_loss         | 3.7e+05  |\n",
      "|    dual_loss           | 695      |\n",
      "|    kl_loss             | 1.27     |\n",
      "|    kl_mean_loss        | 1.26     |\n",
      "|    kl_std_loss         | 0.0063   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 27866    |\n",
      "|    penalty_temperature | 6.94     |\n",
      "|    policy_loss         | 13.6     |\n",
      "|    policy_mean_loss    | 7.24     |\n",
      "|    policy_std_loss     | 6.4      |\n",
      "|    std                 | 0.507    |\n",
      "|    temperature         | 3.85     |\n",
      "|    temperature_loss    | 696      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 51.7     |\n",
      "|    ep_rew_mean         | -115     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 192      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 397      |\n",
      "|    total_timesteps     | 28304    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 1.11e+03 |\n",
      "|    alpha_mean          | 2.08     |\n",
      "|    alpha_mean_loss     | -1.35    |\n",
      "|    alpha_std           | 14.3     |\n",
      "|    alpha_std_loss      | -0.00849 |\n",
      "|    critic_loss         | 7.7e+05  |\n",
      "|    dual_loss           | 1.1e+03  |\n",
      "|    kl_loss             | 1.45     |\n",
      "|    kl_mean_loss        | 1.44     |\n",
      "|    kl_std_loss         | 0.00855  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 28094    |\n",
      "|    penalty_temperature | 7.01     |\n",
      "|    policy_loss         | 14.1     |\n",
      "|    policy_mean_loss    | 7.5      |\n",
      "|    policy_std_loss     | 6.58     |\n",
      "|    std                 | 0.514    |\n",
      "|    temperature         | 3.95     |\n",
      "|    temperature_loss    | 1.1e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 51.8     |\n",
      "|    ep_rew_mean         | -115     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 196      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 401      |\n",
      "|    total_timesteps     | 28574    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 1.58e+03 |\n",
      "|    alpha_mean          | 2.17     |\n",
      "|    alpha_mean_loss     | -1.36    |\n",
      "|    alpha_std           | 14.5     |\n",
      "|    alpha_std_loss      | -0.0134  |\n",
      "|    critic_loss         | 2.08e+06 |\n",
      "|    dual_loss           | 1.57e+03 |\n",
      "|    kl_loss             | 1.46     |\n",
      "|    kl_mean_loss        | 1.45     |\n",
      "|    kl_std_loss         | 0.0134   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 28362    |\n",
      "|    penalty_temperature | 7.11     |\n",
      "|    policy_loss         | 14.8     |\n",
      "|    policy_mean_loss    | 7.82     |\n",
      "|    policy_std_loss     | 6.98     |\n",
      "|    std                 | 0.525    |\n",
      "|    temperature         | 4.07     |\n",
      "|    temperature_loss    | 1.57e+03 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 51.8     |\n",
      "|    ep_rew_mean         | -115     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 200      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 405      |\n",
      "|    total_timesteps     | 28842    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.4e+03  |\n",
      "|    alpha_mean          | 2.25     |\n",
      "|    alpha_mean_loss     | -1.64    |\n",
      "|    alpha_std           | 14.7     |\n",
      "|    alpha_std_loss      | -0.0194  |\n",
      "|    critic_loss         | 2.93e+06 |\n",
      "|    dual_loss           | 2.38e+03 |\n",
      "|    kl_loss             | 1.75     |\n",
      "|    kl_mean_loss        | 1.73     |\n",
      "|    kl_std_loss         | 0.0195   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 28635    |\n",
      "|    penalty_temperature | 7.2      |\n",
      "|    policy_loss         | 15.7     |\n",
      "|    policy_mean_loss    | 8.34     |\n",
      "|    policy_std_loss     | 7.38     |\n",
      "|    std                 | 0.537    |\n",
      "|    temperature         | 4.19     |\n",
      "|    temperature_loss    | 2.38e+03 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 53.2     |\n",
      "|    ep_rew_mean         | -114     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 204      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 409      |\n",
      "|    total_timesteps     | 29157    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.84e+03 |\n",
      "|    alpha_mean          | 2.34     |\n",
      "|    alpha_mean_loss     | -1.71    |\n",
      "|    alpha_std           | 14.8     |\n",
      "|    alpha_std_loss      | -0.0234  |\n",
      "|    critic_loss         | 2.28e+06 |\n",
      "|    dual_loss           | 2.82e+03 |\n",
      "|    kl_loss             | 1.83     |\n",
      "|    kl_mean_loss        | 1.8      |\n",
      "|    kl_std_loss         | 0.0234   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 28942    |\n",
      "|    penalty_temperature | 7.32     |\n",
      "|    policy_loss         | 16.4     |\n",
      "|    policy_mean_loss    | 8.66     |\n",
      "|    policy_std_loss     | 7.73     |\n",
      "|    std                 | 0.554    |\n",
      "|    temperature         | 4.32     |\n",
      "|    temperature_loss    | 2.82e+03 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 53.8     |\n",
      "|    ep_rew_mean         | -114     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 208      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 413      |\n",
      "|    total_timesteps     | 29417    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 3.14e+03 |\n",
      "|    alpha_mean          | 2.42     |\n",
      "|    alpha_mean_loss     | -1.76    |\n",
      "|    alpha_std           | 15       |\n",
      "|    alpha_std_loss      | -0.0246  |\n",
      "|    critic_loss         | 2.47e+06 |\n",
      "|    dual_loss           | 3.12e+03 |\n",
      "|    kl_loss             | 1.88     |\n",
      "|    kl_mean_loss        | 1.86     |\n",
      "|    kl_std_loss         | 0.0246   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 29203    |\n",
      "|    penalty_temperature | 7.42     |\n",
      "|    policy_loss         | 16.8     |\n",
      "|    policy_mean_loss    | 8.87     |\n",
      "|    policy_std_loss     | 7.95     |\n",
      "|    std                 | 0.569    |\n",
      "|    temperature         | 4.43     |\n",
      "|    temperature_loss    | 3.12e+03 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 69.2     |\n",
      "|    ep_rew_mean         | -114     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 212      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 439      |\n",
      "|    total_timesteps     | 31214    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 6.42e+03 |\n",
      "|    alpha_mean          | 2.91     |\n",
      "|    alpha_mean_loss     | -2.15    |\n",
      "|    alpha_std           | 15.6     |\n",
      "|    alpha_std_loss      | -0.0314  |\n",
      "|    critic_loss         | 1.53e+06 |\n",
      "|    dual_loss           | 6.4e+03  |\n",
      "|    kl_loss             | 2.3      |\n",
      "|    kl_mean_loss        | 2.27     |\n",
      "|    kl_std_loss         | 0.0314   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 31017    |\n",
      "|    penalty_temperature | 8.18     |\n",
      "|    policy_loss         | 19.8     |\n",
      "|    policy_mean_loss    | 10.3     |\n",
      "|    policy_std_loss     | 9.45     |\n",
      "|    std                 | 0.681    |\n",
      "|    temperature         | 5.04     |\n",
      "|    temperature_loss    | 6.4e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 70.2     |\n",
      "|    ep_rew_mean         | -114     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 216      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 442      |\n",
      "|    total_timesteps     | 31479    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 6.96e+03 |\n",
      "|    alpha_mean          | 2.99     |\n",
      "|    alpha_mean_loss     | -2.29    |\n",
      "|    alpha_std           | 15.6     |\n",
      "|    alpha_std_loss      | -0.0346  |\n",
      "|    critic_loss         | 1.6e+06  |\n",
      "|    dual_loss           | 6.94e+03 |\n",
      "|    kl_loss             | 2.44     |\n",
      "|    kl_mean_loss        | 2.41     |\n",
      "|    kl_std_loss         | 0.0347   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 31265    |\n",
      "|    penalty_temperature | 8.28     |\n",
      "|    policy_loss         | 20.3     |\n",
      "|    policy_mean_loss    | 10.6     |\n",
      "|    policy_std_loss     | 9.67     |\n",
      "|    std                 | 0.698    |\n",
      "|    temperature         | 5.12     |\n",
      "|    temperature_loss    | 6.94e+03 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 71       |\n",
      "|    ep_rew_mean         | -114     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 220      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 446      |\n",
      "|    total_timesteps     | 31753    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 7.97e+03 |\n",
      "|    alpha_mean          | 3.06     |\n",
      "|    alpha_mean_loss     | -2.21    |\n",
      "|    alpha_std           | 15.7     |\n",
      "|    alpha_std_loss      | -0.0372  |\n",
      "|    critic_loss         | 1.59e+06 |\n",
      "|    dual_loss           | 7.95e+03 |\n",
      "|    kl_loss             | 2.37     |\n",
      "|    kl_mean_loss        | 2.33     |\n",
      "|    kl_std_loss         | 0.0373   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 31554    |\n",
      "|    penalty_temperature | 8.4      |\n",
      "|    policy_loss         | 20.5     |\n",
      "|    policy_mean_loss    | 10.7     |\n",
      "|    policy_std_loss     | 9.83     |\n",
      "|    std                 | 0.718    |\n",
      "|    temperature         | 5.21     |\n",
      "|    temperature_loss    | 7.95e+03 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 75       |\n",
      "|    ep_rew_mean         | -114     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 224      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 452      |\n",
      "|    total_timesteps     | 32351    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 9.42e+03 |\n",
      "|    alpha_mean          | 3.16     |\n",
      "|    alpha_mean_loss     | -2.32    |\n",
      "|    alpha_std           | 15.8     |\n",
      "|    alpha_std_loss      | -0.0393  |\n",
      "|    critic_loss         | 1.9e+06  |\n",
      "|    dual_loss           | 9.39e+03 |\n",
      "|    kl_loss             | 2.49     |\n",
      "|    kl_mean_loss        | 2.45     |\n",
      "|    kl_std_loss         | 0.0394   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 31971    |\n",
      "|    penalty_temperature | 8.54     |\n",
      "|    policy_loss         | 21       |\n",
      "|    policy_mean_loss    | 10.9     |\n",
      "|    policy_std_loss     | 10.1     |\n",
      "|    std                 | 0.748    |\n",
      "|    temperature         | 5.32     |\n",
      "|    temperature_loss    | 9.4e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 82.6     |\n",
      "|    ep_rew_mean         | -115     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 228      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 466      |\n",
      "|    total_timesteps     | 33271    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 1.42e+04 |\n",
      "|    alpha_mean          | 3.39     |\n",
      "|    alpha_mean_loss     | -2.45    |\n",
      "|    alpha_std           | 15.9     |\n",
      "|    alpha_std_loss      | -0.0384  |\n",
      "|    critic_loss         | 1.71e+06 |\n",
      "|    dual_loss           | 1.42e+04 |\n",
      "|    kl_loss             | 2.63     |\n",
      "|    kl_mean_loss        | 2.59     |\n",
      "|    kl_std_loss         | 0.0385   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 32883    |\n",
      "|    penalty_temperature | 8.89     |\n",
      "|    policy_loss         | 22       |\n",
      "|    policy_mean_loss    | 11.5     |\n",
      "|    policy_std_loss     | 10.6     |\n",
      "|    std                 | 0.817    |\n",
      "|    temperature         | 5.59     |\n",
      "|    temperature_loss    | 1.42e+04 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 91.4     |\n",
      "|    ep_rew_mean         | -116     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 232      |\n",
      "|    fps                 | 70       |\n",
      "|    time_elapsed        | 484      |\n",
      "|    total_timesteps     | 34338    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.27e+04 |\n",
      "|    alpha_mean          | 3.7      |\n",
      "|    alpha_mean_loss     | -2.49    |\n",
      "|    alpha_std           | 16.1     |\n",
      "|    alpha_std_loss      | -0.0329  |\n",
      "|    critic_loss         | 2.27e+06 |\n",
      "|    dual_loss           | 2.27e+04 |\n",
      "|    kl_loss             | 2.67     |\n",
      "|    kl_mean_loss        | 2.64     |\n",
      "|    kl_std_loss         | 0.033    |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 34118    |\n",
      "|    penalty_temperature | 9.37     |\n",
      "|    policy_loss         | 23.3     |\n",
      "|    policy_mean_loss    | 12.1     |\n",
      "|    policy_std_loss     | 11.2     |\n",
      "|    std                 | 0.925    |\n",
      "|    temperature         | 5.96     |\n",
      "|    temperature_loss    | 2.27e+04 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 94.1     |\n",
      "|    ep_rew_mean         | -116     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 236      |\n",
      "|    fps                 | 70       |\n",
      "|    time_elapsed        | 491      |\n",
      "|    total_timesteps     | 34804    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.47e+04 |\n",
      "|    alpha_mean          | 3.83     |\n",
      "|    alpha_mean_loss     | -2.66    |\n",
      "|    alpha_std           | 16.1     |\n",
      "|    alpha_std_loss      | -0.0244  |\n",
      "|    critic_loss         | 2.39e+06 |\n",
      "|    dual_loss           | 2.47e+04 |\n",
      "|    kl_loss             | 2.83     |\n",
      "|    kl_mean_loss        | 2.81     |\n",
      "|    kl_std_loss         | 0.0245   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 34619    |\n",
      "|    penalty_temperature | 9.57     |\n",
      "|    policy_loss         | 23.9     |\n",
      "|    policy_mean_loss    | 12.4     |\n",
      "|    policy_std_loss     | 11.5     |\n",
      "|    std                 | 0.97     |\n",
      "|    temperature         | 6.11     |\n",
      "|    temperature_loss    | 2.47e+04 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 101      |\n",
      "|    ep_rew_mean         | -117     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 240      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 501      |\n",
      "|    total_timesteps     | 35656    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.41e+04 |\n",
      "|    alpha_mean          | 4        |\n",
      "|    alpha_mean_loss     | -2.57    |\n",
      "|    alpha_std           | 16.3     |\n",
      "|    alpha_std_loss      | -0.0179  |\n",
      "|    critic_loss         | 2.73e+06 |\n",
      "|    dual_loss           | 2.41e+04 |\n",
      "|    kl_loss             | 2.75     |\n",
      "|    kl_mean_loss        | 2.73     |\n",
      "|    kl_std_loss         | 0.018    |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 35319    |\n",
      "|    penalty_temperature | 9.83     |\n",
      "|    policy_loss         | 24.5     |\n",
      "|    policy_mean_loss    | 12.7     |\n",
      "|    policy_std_loss     | 11.8     |\n",
      "|    std                 | 1.02     |\n",
      "|    temperature         | 6.31     |\n",
      "|    temperature_loss    | 2.41e+04 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 107      |\n",
      "|    ep_rew_mean         | -118     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 244      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 512      |\n",
      "|    total_timesteps     | 36402    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.37e+04 |\n",
      "|    alpha_mean          | 4.2      |\n",
      "|    alpha_mean_loss     | -2.71    |\n",
      "|    alpha_std           | 16.5     |\n",
      "|    alpha_std_loss      | -0.0123  |\n",
      "|    critic_loss         | 4.2e+06  |\n",
      "|    dual_loss           | 2.37e+04 |\n",
      "|    kl_loss             | 2.89     |\n",
      "|    kl_mean_loss        | 2.88     |\n",
      "|    kl_std_loss         | 0.0124   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 36016    |\n",
      "|    penalty_temperature | 10.1     |\n",
      "|    policy_loss         | 25.2     |\n",
      "|    policy_mean_loss    | 13       |\n",
      "|    policy_std_loss     | 12.1     |\n",
      "|    std                 | 1.07     |\n",
      "|    temperature         | 6.55     |\n",
      "|    temperature_loss    | 2.37e+04 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 111      |\n",
      "|    ep_rew_mean         | -118     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 248      |\n",
      "|    fps                 | 70       |\n",
      "|    time_elapsed        | 522      |\n",
      "|    total_timesteps     | 37002    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.41e+04 |\n",
      "|    alpha_mean          | 4.36     |\n",
      "|    alpha_mean_loss     | -3.03    |\n",
      "|    alpha_std           | 16.7     |\n",
      "|    alpha_std_loss      | -0.0133  |\n",
      "|    critic_loss         | 6.96e+06 |\n",
      "|    dual_loss           | 2.41e+04 |\n",
      "|    kl_loss             | 3.21     |\n",
      "|    kl_mean_loss        | 3.2      |\n",
      "|    kl_std_loss         | 0.0134   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 36666    |\n",
      "|    penalty_temperature | 10.3     |\n",
      "|    policy_loss         | 25.7     |\n",
      "|    policy_mean_loss    | 13.3     |\n",
      "|    policy_std_loss     | 12.3     |\n",
      "|    std                 | 1.11     |\n",
      "|    temperature         | 6.75     |\n",
      "|    temperature_loss    | 2.41e+04 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 129      |\n",
      "|    ep_rew_mean         | -119     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 252      |\n",
      "|    fps                 | 73       |\n",
      "|    time_elapsed        | 530      |\n",
      "|    total_timesteps     | 38988    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.32e+04 |\n",
      "|    alpha_mean          | 4.49     |\n",
      "|    alpha_mean_loss     | -2.65    |\n",
      "|    alpha_std           | 16.8     |\n",
      "|    alpha_std_loss      | -0.0135  |\n",
      "|    critic_loss         | 3.27e+06 |\n",
      "|    dual_loss           | 2.31e+04 |\n",
      "|    kl_loss             | 2.84     |\n",
      "|    kl_mean_loss        | 2.83     |\n",
      "|    kl_std_loss         | 0.0136   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 37247    |\n",
      "|    penalty_temperature | 10.5     |\n",
      "|    policy_loss         | 26       |\n",
      "|    policy_mean_loss    | 13.5     |\n",
      "|    policy_std_loss     | 12.6     |\n",
      "|    std                 | 1.14     |\n",
      "|    temperature         | 6.91     |\n",
      "|    temperature_loss    | 2.31e+04 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 176      |\n",
      "|    ep_rew_mean         | -121     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 256      |\n",
      "|    fps                 | 73       |\n",
      "|    time_elapsed        | 601      |\n",
      "|    total_timesteps     | 43917    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 4.3e+04  |\n",
      "|    alpha_mean          | 5.8      |\n",
      "|    alpha_mean_loss     | -3.44    |\n",
      "|    alpha_std           | 17.7     |\n",
      "|    alpha_std_loss      | -0.0183  |\n",
      "|    critic_loss         | 1.19e+08 |\n",
      "|    dual_loss           | 4.3e+04  |\n",
      "|    kl_loss             | 3.69     |\n",
      "|    kl_mean_loss        | 3.67     |\n",
      "|    kl_std_loss         | 0.0184   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 42176    |\n",
      "|    penalty_temperature | 12       |\n",
      "|    policy_loss         | 30.1     |\n",
      "|    policy_mean_loss    | 15.6     |\n",
      "|    policy_std_loss     | 14.5     |\n",
      "|    std                 | 1.49     |\n",
      "|    temperature         | 8.39     |\n",
      "|    temperature_loss    | 4.3e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 203      |\n",
      "|    ep_rew_mean         | -123     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 260      |\n",
      "|    fps                 | 72       |\n",
      "|    time_elapsed        | 642      |\n",
      "|    total_timesteps     | 46784    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 7.87e+04 |\n",
      "|    alpha_mean          | 6.57     |\n",
      "|    alpha_mean_loss     | -2.89    |\n",
      "|    alpha_std           | 18.7     |\n",
      "|    alpha_std_loss      | -0.0141  |\n",
      "|    critic_loss         | 7.76e+07 |\n",
      "|    dual_loss           | 7.87e+04 |\n",
      "|    kl_loss             | 3.16     |\n",
      "|    kl_mean_loss        | 3.15     |\n",
      "|    kl_std_loss         | 0.0142   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 45043    |\n",
      "|    penalty_temperature | 13       |\n",
      "|    policy_loss         | 32.8     |\n",
      "|    policy_mean_loss    | 16.8     |\n",
      "|    policy_std_loss     | 16       |\n",
      "|    std                 | 1.85     |\n",
      "|    temperature         | 9.27     |\n",
      "|    temperature_loss    | 7.87e+04 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 250      |\n",
      "|    ep_rew_mean         | -125     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 264      |\n",
      "|    fps                 | 72       |\n",
      "|    time_elapsed        | 714      |\n",
      "|    total_timesteps     | 51676    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 6.66e+05 |\n",
      "|    alpha_mean          | 7.57     |\n",
      "|    alpha_mean_loss     | -3.39    |\n",
      "|    alpha_std           | 20       |\n",
      "|    alpha_std_loss      | -0.0202  |\n",
      "|    critic_loss         | 2.16e+10 |\n",
      "|    dual_loss           | 6.66e+05 |\n",
      "|    kl_loss             | 3.71     |\n",
      "|    kl_mean_loss        | 3.69     |\n",
      "|    kl_std_loss         | 0.0203   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 49935    |\n",
      "|    penalty_temperature | 14.4     |\n",
      "|    policy_loss         | 37.8     |\n",
      "|    policy_mean_loss    | 19.3     |\n",
      "|    policy_std_loss     | 18.5     |\n",
      "|    std                 | 2.91     |\n",
      "|    temperature         | 10.5     |\n",
      "|    temperature_loss    | 6.66e+05 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 252      |\n",
      "|    ep_rew_mean         | -125     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 268      |\n",
      "|    fps                 | 70       |\n",
      "|    time_elapsed        | 741      |\n",
      "|    total_timesteps     | 52105    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 4.31e+06 |\n",
      "|    alpha_mean          | 8.18     |\n",
      "|    alpha_mean_loss     | -4.93    |\n",
      "|    alpha_std           | 20.7     |\n",
      "|    alpha_std_loss      | -0.0191  |\n",
      "|    critic_loss         | 1.62e+12 |\n",
      "|    dual_loss           | 4.31e+06 |\n",
      "|    kl_loss             | 5.28     |\n",
      "|    kl_mean_loss        | 5.26     |\n",
      "|    kl_std_loss         | 0.0192   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 51847    |\n",
      "|    penalty_temperature | 15.3     |\n",
      "|    policy_loss         | 40.4     |\n",
      "|    policy_mean_loss    | 20.7     |\n",
      "|    policy_std_loss     | 19.7     |\n",
      "|    std                 | 3.38     |\n",
      "|    temperature         | 11.1     |\n",
      "|    temperature_loss    | 4.31e+06 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 256      |\n",
      "|    ep_rew_mean         | -125     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 272      |\n",
      "|    fps                 | 70       |\n",
      "|    time_elapsed        | 749      |\n",
      "|    total_timesteps     | 52665    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 5.83e+06 |\n",
      "|    alpha_mean          | 8.32     |\n",
      "|    alpha_mean_loss     | -3.9     |\n",
      "|    alpha_std           | 20.8     |\n",
      "|    alpha_std_loss      | -0.0142  |\n",
      "|    critic_loss         | 1.7e+12  |\n",
      "|    dual_loss           | 5.83e+06 |\n",
      "|    kl_loss             | 4.24     |\n",
      "|    kl_mean_loss        | 4.23     |\n",
      "|    kl_std_loss         | 0.0143   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 52414    |\n",
      "|    penalty_temperature | 15.5     |\n",
      "|    policy_loss         | 40.5     |\n",
      "|    policy_mean_loss    | 20.6     |\n",
      "|    policy_std_loss     | 19.9     |\n",
      "|    std                 | 3.49     |\n",
      "|    temperature         | 11.1     |\n",
      "|    temperature_loss    | 5.83e+06 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 273      |\n",
      "|    ep_rew_mean         | -126     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 276      |\n",
      "|    fps                 | 70       |\n",
      "|    time_elapsed        | 779      |\n",
      "|    total_timesteps     | 54639    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 6.86e+06 |\n",
      "|    alpha_mean          | 8.62     |\n",
      "|    alpha_mean_loss     | -4.39    |\n",
      "|    alpha_std           | 21       |\n",
      "|    alpha_std_loss      | -0.00951 |\n",
      "|    critic_loss         | 9.74e+11 |\n",
      "|    dual_loss           | 6.86e+06 |\n",
      "|    kl_loss             | 4.74     |\n",
      "|    kl_mean_loss        | 4.73     |\n",
      "|    kl_std_loss         | 0.00959  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 54446    |\n",
      "|    penalty_temperature | 16       |\n",
      "|    policy_loss         | 41.4     |\n",
      "|    policy_mean_loss    | 21.1     |\n",
      "|    policy_std_loss     | 20.3     |\n",
      "|    std                 | 3.82     |\n",
      "|    temperature         | 11.2     |\n",
      "|    temperature_loss    | 6.86e+06 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 273      |\n",
      "|    ep_rew_mean         | -126     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 280      |\n",
      "|    fps                 | 70       |\n",
      "|    time_elapsed        | 782      |\n",
      "|    total_timesteps     | 54890    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 6.79e+06 |\n",
      "|    alpha_mean          | 8.84     |\n",
      "|    alpha_mean_loss     | -4.86    |\n",
      "|    alpha_std           | 21.2     |\n",
      "|    alpha_std_loss      | -0.0062  |\n",
      "|    critic_loss         | 1.08e+12 |\n",
      "|    dual_loss           | 6.79e+06 |\n",
      "|    kl_loss             | 5.22     |\n",
      "|    kl_mean_loss        | 5.21     |\n",
      "|    kl_std_loss         | 0.00628  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 54686    |\n",
      "|    penalty_temperature | 16.3     |\n",
      "|    policy_loss         | 41.9     |\n",
      "|    policy_mean_loss    | 21.4     |\n",
      "|    policy_std_loss     | 20.5     |\n",
      "|    std                 | 3.85     |\n",
      "|    temperature         | 11.2     |\n",
      "|    temperature_loss    | 6.79e+06 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 274      |\n",
      "|    ep_rew_mean         | -127     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 284      |\n",
      "|    fps                 | 70       |\n",
      "|    time_elapsed        | 787      |\n",
      "|    total_timesteps     | 55223    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 6.94e+06 |\n",
      "|    alpha_mean          | 8.93     |\n",
      "|    alpha_mean_loss     | -6.43    |\n",
      "|    alpha_std           | 21.3     |\n",
      "|    alpha_std_loss      | -0.00628 |\n",
      "|    critic_loss         | 9.72e+11 |\n",
      "|    dual_loss           | 6.94e+06 |\n",
      "|    kl_loss             | 6.79     |\n",
      "|    kl_mean_loss        | 6.78     |\n",
      "|    kl_std_loss         | 0.00636  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 55035    |\n",
      "|    penalty_temperature | 16.4     |\n",
      "|    policy_loss         | 42.3     |\n",
      "|    policy_mean_loss    | 21.8     |\n",
      "|    policy_std_loss     | 20.5     |\n",
      "|    std                 | 3.88     |\n",
      "|    temperature         | 11.3     |\n",
      "|    temperature_loss    | 6.94e+06 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 274      |\n",
      "|    ep_rew_mean         | -127     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 288      |\n",
      "|    fps                 | 70       |\n",
      "|    time_elapsed        | 791      |\n",
      "|    total_timesteps     | 55461    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 7.16e+06 |\n",
      "|    alpha_mean          | 9        |\n",
      "|    alpha_mean_loss     | -4.91    |\n",
      "|    alpha_std           | 21.3     |\n",
      "|    alpha_std_loss      | -0.00676 |\n",
      "|    critic_loss         | 7.92e+11 |\n",
      "|    dual_loss           | 7.16e+06 |\n",
      "|    kl_loss             | 5.28     |\n",
      "|    kl_mean_loss        | 5.27     |\n",
      "|    kl_std_loss         | 0.00684  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 55273    |\n",
      "|    penalty_temperature | 16.4     |\n",
      "|    policy_loss         | 42.2     |\n",
      "|    policy_mean_loss    | 21.5     |\n",
      "|    policy_std_loss     | 20.6     |\n",
      "|    std                 | 3.91     |\n",
      "|    temperature         | 11.3     |\n",
      "|    temperature_loss    | 7.16e+06 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 290      |\n",
      "|    ep_rew_mean         | -128     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 292      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 795      |\n",
      "|    total_timesteps     | 57261    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 7.35e+06 |\n",
      "|    alpha_mean          | 9.06     |\n",
      "|    alpha_mean_loss     | -5       |\n",
      "|    alpha_std           | 21.4     |\n",
      "|    alpha_std_loss      | -0.00628 |\n",
      "|    critic_loss         | 9.42e+11 |\n",
      "|    dual_loss           | 7.35e+06 |\n",
      "|    kl_loss             | 5.37     |\n",
      "|    kl_mean_loss        | 5.36     |\n",
      "|    kl_std_loss         | 0.00636  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 55520    |\n",
      "|    penalty_temperature | 16.5     |\n",
      "|    policy_loss         | 42.3     |\n",
      "|    policy_mean_loss    | 21.6     |\n",
      "|    policy_std_loss     | 20.7     |\n",
      "|    std                 | 3.94     |\n",
      "|    temperature         | 11.3     |\n",
      "|    temperature_loss    | 7.35e+06 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 308      |\n",
      "|    ep_rew_mean         | -128     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 296      |\n",
      "|    fps                 | 71       |\n",
      "|    time_elapsed        | 826      |\n",
      "|    total_timesteps     | 59345    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 1.15e+07 |\n",
      "|    alpha_mean          | 9.62     |\n",
      "|    alpha_mean_loss     | -8.59    |\n",
      "|    alpha_std           | 21.8     |\n",
      "|    alpha_std_loss      | -0.00732 |\n",
      "|    critic_loss         | 2.03e+12 |\n",
      "|    dual_loss           | 1.15e+07 |\n",
      "|    kl_loss             | 8.98     |\n",
      "|    kl_mean_loss        | 8.97     |\n",
      "|    kl_std_loss         | 0.0074   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 57604    |\n",
      "|    penalty_temperature | 17.2     |\n",
      "|    policy_loss         | 43.9     |\n",
      "|    policy_mean_loss    | 22.7     |\n",
      "|    policy_std_loss     | 21.2     |\n",
      "|    std                 | 4.29     |\n",
      "|    temperature         | 11.4     |\n",
      "|    temperature_loss    | 1.15e+07 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 312      |\n",
      "|    ep_rew_mean         | -129     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 300      |\n",
      "|    fps                 | 70       |\n",
      "|    time_elapsed        | 851      |\n",
      "|    total_timesteps     | 60024    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 1.72e+07 |\n",
      "|    alpha_mean          | 10.1     |\n",
      "|    alpha_mean_loss     | -10.4    |\n",
      "|    alpha_std           | 22.3     |\n",
      "|    alpha_std_loss      | -0.00488 |\n",
      "|    critic_loss         | 3.9e+12  |\n",
      "|    dual_loss           | 1.72e+07 |\n",
      "|    kl_loss             | 10.8     |\n",
      "|    kl_mean_loss        | 10.8     |\n",
      "|    kl_std_loss         | 0.00497  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 59382    |\n",
      "|    penalty_temperature | 17.8     |\n",
      "|    policy_loss         | 44.7     |\n",
      "|    policy_mean_loss    | 23.2     |\n",
      "|    policy_std_loss     | 21.4     |\n",
      "|    std                 | 4.52     |\n",
      "|    temperature         | 11.4     |\n",
      "|    temperature_loss    | 1.72e+07 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 317      |\n",
      "|    ep_rew_mean         | -129     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 304      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 870      |\n",
      "|    total_timesteps     | 60864    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.25e+07 |\n",
      "|    alpha_mean          | 10.5     |\n",
      "|    alpha_mean_loss     | -13.6    |\n",
      "|    alpha_std           | 22.5     |\n",
      "|    alpha_std_loss      | -0.0044  |\n",
      "|    critic_loss         | 8.96e+12 |\n",
      "|    dual_loss           | 2.25e+07 |\n",
      "|    kl_loss             | 14       |\n",
      "|    kl_mean_loss        | 14       |\n",
      "|    kl_std_loss         | 0.00449  |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 60625    |\n",
      "|    penalty_temperature | 18.1     |\n",
      "|    policy_loss         | 45.7     |\n",
      "|    policy_mean_loss    | 24       |\n",
      "|    policy_std_loss     | 21.7     |\n",
      "|    std                 | 4.63     |\n",
      "|    temperature         | 11.4     |\n",
      "|    temperature_loss    | 2.25e+07 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 323      |\n",
      "|    ep_rew_mean         | -130     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 308      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 881      |\n",
      "|    total_timesteps     | 61711    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.82e+07 |\n",
      "|    alpha_mean          | 10.7     |\n",
      "|    alpha_mean_loss     | -15      |\n",
      "|    alpha_std           | 22.6     |\n",
      "|    alpha_std_loss      | -0.0107  |\n",
      "|    critic_loss         | 2.77e+13 |\n",
      "|    dual_loss           | 2.82e+07 |\n",
      "|    kl_loss             | 15.4     |\n",
      "|    kl_mean_loss        | 15.4     |\n",
      "|    kl_std_loss         | 0.0108   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 61398    |\n",
      "|    penalty_temperature | 18.3     |\n",
      "|    policy_loss         | 46.4     |\n",
      "|    policy_mean_loss    | 24.5     |\n",
      "|    policy_std_loss     | 21.9     |\n",
      "|    std                 | 4.64     |\n",
      "|    temperature         | 11.4     |\n",
      "|    temperature_loss    | 2.82e+07 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 311      |\n",
      "|    ep_rew_mean         | -130     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 312      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 891      |\n",
      "|    total_timesteps     | 62312    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 4.09e+07 |\n",
      "|    alpha_mean          | 10.9     |\n",
      "|    alpha_mean_loss     | -15.7    |\n",
      "|    alpha_std           | 22.9     |\n",
      "|    alpha_std_loss      | -0.017   |\n",
      "|    critic_loss         | 1.49e+14 |\n",
      "|    dual_loss           | 4.09e+07 |\n",
      "|    kl_loss             | 16.2     |\n",
      "|    kl_mean_loss        | 16.1     |\n",
      "|    kl_std_loss         | 0.0171   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 62035    |\n",
      "|    penalty_temperature | 18.5     |\n",
      "|    policy_loss         | 47.4     |\n",
      "|    policy_mean_loss    | 25       |\n",
      "|    policy_std_loss     | 22.4     |\n",
      "|    std                 | 4.66     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 4.09e+07 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 314      |\n",
      "|    ep_rew_mean         | -131     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 316      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 899      |\n",
      "|    total_timesteps     | 62861    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 6.87e+07 |\n",
      "|    alpha_mean          | 11.1     |\n",
      "|    alpha_mean_loss     | -14.3    |\n",
      "|    alpha_std           | 23.1     |\n",
      "|    alpha_std_loss      | -0.0206  |\n",
      "|    critic_loss         | 8.26e+14 |\n",
      "|    dual_loss           | 6.87e+07 |\n",
      "|    kl_loss             | 14.7     |\n",
      "|    kl_mean_loss        | 14.7     |\n",
      "|    kl_std_loss         | 0.0206   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 62600    |\n",
      "|    penalty_temperature | 18.7     |\n",
      "|    policy_loss         | 48.3     |\n",
      "|    policy_mean_loss    | 25.3     |\n",
      "|    policy_std_loss     | 23       |\n",
      "|    std                 | 4.75     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 6.87e+07 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 316      |\n",
      "|    ep_rew_mean         | -131     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 320      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 907      |\n",
      "|    total_timesteps     | 63370    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 1.15e+08 |\n",
      "|    alpha_mean          | 11.2     |\n",
      "|    alpha_mean_loss     | -14.3    |\n",
      "|    alpha_std           | 23.3     |\n",
      "|    alpha_std_loss      | -0.0224  |\n",
      "|    critic_loss         | 2.91e+15 |\n",
      "|    dual_loss           | 1.15e+08 |\n",
      "|    kl_loss             | 14.8     |\n",
      "|    kl_mean_loss        | 14.8     |\n",
      "|    kl_std_loss         | 0.0225   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 63114    |\n",
      "|    penalty_temperature | 18.8     |\n",
      "|    policy_loss         | 48.9     |\n",
      "|    policy_mean_loss    | 25.6     |\n",
      "|    policy_std_loss     | 23.3     |\n",
      "|    std                 | 4.88     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 1.15e+08 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 314      |\n",
      "|    ep_rew_mean         | -131     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 324      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 913      |\n",
      "|    total_timesteps     | 63780    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 1.98e+08 |\n",
      "|    alpha_mean          | 11.4     |\n",
      "|    alpha_mean_loss     | -15      |\n",
      "|    alpha_std           | 23.4     |\n",
      "|    alpha_std_loss      | -0.0242  |\n",
      "|    critic_loss         | 9.84e+15 |\n",
      "|    dual_loss           | 1.98e+08 |\n",
      "|    kl_loss             | 15.4     |\n",
      "|    kl_mean_loss        | 15.4     |\n",
      "|    kl_std_loss         | 0.0243   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 63508    |\n",
      "|    penalty_temperature | 18.9     |\n",
      "|    policy_loss         | 49.5     |\n",
      "|    policy_mean_loss    | 26       |\n",
      "|    policy_std_loss     | 23.5     |\n",
      "|    std                 | 4.99     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 1.98e+08 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 311      |\n",
      "|    ep_rew_mean         | -130     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 328      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 921      |\n",
      "|    total_timesteps     | 64377    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 3.7e+08  |\n",
      "|    alpha_mean          | 11.5     |\n",
      "|    alpha_mean_loss     | -15.3    |\n",
      "|    alpha_std           | 23.6     |\n",
      "|    alpha_std_loss      | -0.0268  |\n",
      "|    critic_loss         | 3.61e+16 |\n",
      "|    dual_loss           | 3.7e+08  |\n",
      "|    kl_loss             | 15.8     |\n",
      "|    kl_mean_loss        | 15.8     |\n",
      "|    kl_std_loss         | 0.0269   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 64053    |\n",
      "|    penalty_temperature | 19.1     |\n",
      "|    policy_loss         | 50       |\n",
      "|    policy_mean_loss    | 26.3     |\n",
      "|    policy_std_loss     | 23.8     |\n",
      "|    std                 | 5.13     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 3.7e+08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 307      |\n",
      "|    ep_rew_mean         | -129     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 332      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 929      |\n",
      "|    total_timesteps     | 64992    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 7.78e+08 |\n",
      "|    alpha_mean          | 11.7     |\n",
      "|    alpha_mean_loss     | -15      |\n",
      "|    alpha_std           | 23.7     |\n",
      "|    alpha_std_loss      | -0.0284  |\n",
      "|    critic_loss         | 1.61e+17 |\n",
      "|    dual_loss           | 7.78e+08 |\n",
      "|    kl_loss             | 15.5     |\n",
      "|    kl_mean_loss        | 15.4     |\n",
      "|    kl_std_loss         | 0.0285   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 64573    |\n",
      "|    penalty_temperature | 19.3     |\n",
      "|    policy_loss         | 50.5     |\n",
      "|    policy_mean_loss    | 26.5     |\n",
      "|    policy_std_loss     | 24       |\n",
      "|    std                 | 5.27     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 7.78e+08 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 306      |\n",
      "|    ep_rew_mean         | -129     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 336      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 937      |\n",
      "|    total_timesteps     | 65380    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 1.46e+09 |\n",
      "|    alpha_mean          | 11.8     |\n",
      "|    alpha_mean_loss     | -15.4    |\n",
      "|    alpha_std           | 23.8     |\n",
      "|    alpha_std_loss      | -0.03    |\n",
      "|    critic_loss         | 6.38e+17 |\n",
      "|    dual_loss           | 1.46e+09 |\n",
      "|    kl_loss             | 15.9     |\n",
      "|    kl_mean_loss        | 15.9     |\n",
      "|    kl_std_loss         | 0.0301   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 65126    |\n",
      "|    penalty_temperature | 19.4     |\n",
      "|    policy_loss         | 51.1     |\n",
      "|    policy_mean_loss    | 26.8     |\n",
      "|    policy_std_loss     | 24.3     |\n",
      "|    std                 | 5.43     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 1.46e+09 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 302      |\n",
      "|    ep_rew_mean         | -129     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 340      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 943      |\n",
      "|    total_timesteps     | 65839    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.47e+09 |\n",
      "|    alpha_mean          | 12       |\n",
      "|    alpha_mean_loss     | -14.5    |\n",
      "|    alpha_std           | 23.9     |\n",
      "|    alpha_std_loss      | -0.0316  |\n",
      "|    critic_loss         | 2.08e+18 |\n",
      "|    dual_loss           | 2.47e+09 |\n",
      "|    kl_loss             | 15       |\n",
      "|    kl_mean_loss        | 15       |\n",
      "|    kl_std_loss         | 0.0317   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 65584    |\n",
      "|    penalty_temperature | 19.6     |\n",
      "|    policy_loss         | 51.4     |\n",
      "|    policy_mean_loss    | 26.8     |\n",
      "|    policy_std_loss     | 24.5     |\n",
      "|    std                 | 5.56     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 2.47e+09 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 300      |\n",
      "|    ep_rew_mean         | -128     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 344      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 950      |\n",
      "|    total_timesteps     | 66357    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 4.16e+09 |\n",
      "|    alpha_mean          | 12.1     |\n",
      "|    alpha_mean_loss     | -15.1    |\n",
      "|    alpha_std           | 24       |\n",
      "|    alpha_std_loss      | -0.0318  |\n",
      "|    critic_loss         | 6.32e+18 |\n",
      "|    dual_loss           | 4.16e+09 |\n",
      "|    kl_loss             | 15.6     |\n",
      "|    kl_mean_loss        | 15.6     |\n",
      "|    kl_std_loss         | 0.0319   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 66019    |\n",
      "|    penalty_temperature | 19.7     |\n",
      "|    policy_loss         | 51.8     |\n",
      "|    policy_mean_loss    | 27.1     |\n",
      "|    policy_std_loss     | 24.7     |\n",
      "|    std                 | 5.69     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 4.16e+09 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 298      |\n",
      "|    ep_rew_mean         | -128     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 348      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 958      |\n",
      "|    total_timesteps     | 66809    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 7.41e+09 |\n",
      "|    alpha_mean          | 12.2     |\n",
      "|    alpha_mean_loss     | -14.9    |\n",
      "|    alpha_std           | 24.1     |\n",
      "|    alpha_std_loss      | -0.0349  |\n",
      "|    critic_loss         | 2.31e+19 |\n",
      "|    dual_loss           | 7.41e+09 |\n",
      "|    kl_loss             | 15.5     |\n",
      "|    kl_mean_loss        | 15.4     |\n",
      "|    kl_std_loss         | 0.035    |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 66540    |\n",
      "|    penalty_temperature | 19.9     |\n",
      "|    policy_loss         | 52.2     |\n",
      "|    policy_mean_loss    | 27.3     |\n",
      "|    policy_std_loss     | 24.9     |\n",
      "|    std                 | 5.84     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 7.41e+09 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 284      |\n",
      "|    ep_rew_mean         | -128     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 352      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 967      |\n",
      "|    total_timesteps     | 67384    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 1.37e+10 |\n",
      "|    alpha_mean          | 12.4     |\n",
      "|    alpha_mean_loss     | -18.6    |\n",
      "|    alpha_std           | 24.2     |\n",
      "|    alpha_std_loss      | -0.038   |\n",
      "|    critic_loss         | 9.49e+19 |\n",
      "|    dual_loss           | 1.37e+10 |\n",
      "|    kl_loss             | 19.1     |\n",
      "|    kl_mean_loss        | 19.1     |\n",
      "|    kl_std_loss         | 0.0381   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 67168    |\n",
      "|    penalty_temperature | 20.1     |\n",
      "|    policy_loss         | 53.4     |\n",
      "|    policy_mean_loss    | 28.2     |\n",
      "|    policy_std_loss     | 25.2     |\n",
      "|    std                 | 6.03     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 1.37e+10 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 241      |\n",
      "|    ep_rew_mean         | -127     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 356      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 975      |\n",
      "|    total_timesteps     | 67992    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.28e+10 |\n",
      "|    alpha_mean          | 12.6     |\n",
      "|    alpha_mean_loss     | -20      |\n",
      "|    alpha_std           | 24.2     |\n",
      "|    alpha_std_loss      | -0.0383  |\n",
      "|    critic_loss         | 2.94e+20 |\n",
      "|    dual_loss           | 2.28e+10 |\n",
      "|    kl_loss             | 20.6     |\n",
      "|    kl_mean_loss        | 20.5     |\n",
      "|    kl_std_loss         | 0.0384   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 67716    |\n",
      "|    penalty_temperature | 20.2     |\n",
      "|    policy_loss         | 54.1     |\n",
      "|    policy_mean_loss    | 28.6     |\n",
      "|    policy_std_loss     | 25.5     |\n",
      "|    std                 | 6.22     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 2.28e+10 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 217      |\n",
      "|    ep_rew_mean         | -125     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 360      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 982      |\n",
      "|    total_timesteps     | 68455    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 3.37e+10 |\n",
      "|    alpha_mean          | 12.7     |\n",
      "|    alpha_mean_loss     | -19      |\n",
      "|    alpha_std           | 24.3     |\n",
      "|    alpha_std_loss      | -0.0379  |\n",
      "|    critic_loss         | 7.13e+20 |\n",
      "|    dual_loss           | 3.37e+10 |\n",
      "|    kl_loss             | 19.6     |\n",
      "|    kl_mean_loss        | 19.5     |\n",
      "|    kl_std_loss         | 0.038    |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 68161    |\n",
      "|    penalty_temperature | 20.4     |\n",
      "|    policy_loss         | 54.3     |\n",
      "|    policy_mean_loss    | 28.7     |\n",
      "|    policy_std_loss     | 25.7     |\n",
      "|    std                 | 6.37     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 3.37e+10 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 173      |\n",
      "|    ep_rew_mean         | -124     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 364      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 990      |\n",
      "|    total_timesteps     | 68964    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 5.1e+10  |\n",
      "|    alpha_mean          | 12.8     |\n",
      "|    alpha_mean_loss     | -18      |\n",
      "|    alpha_std           | 24.4     |\n",
      "|    alpha_std_loss      | -0.0365  |\n",
      "|    critic_loss         | 1.75e+21 |\n",
      "|    dual_loss           | 5.1e+10  |\n",
      "|    kl_loss             | 18.6     |\n",
      "|    kl_mean_loss        | 18.6     |\n",
      "|    kl_std_loss         | 0.0366   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 68709    |\n",
      "|    penalty_temperature | 20.6     |\n",
      "|    policy_loss         | 54.7     |\n",
      "|    policy_mean_loss    | 28.7     |\n",
      "|    policy_std_loss     | 25.9     |\n",
      "|    std                 | 6.59     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 5.1e+10  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 174      |\n",
      "|    ep_rew_mean         | -124     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 368      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 999      |\n",
      "|    total_timesteps     | 69501    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 7.56e+10 |\n",
      "|    alpha_mean          | 13       |\n",
      "|    alpha_mean_loss     | -17.8    |\n",
      "|    alpha_std           | 24.5     |\n",
      "|    alpha_std_loss      | -0.0345  |\n",
      "|    critic_loss         | 4.04e+21 |\n",
      "|    dual_loss           | 7.56e+10 |\n",
      "|    kl_loss             | 18.4     |\n",
      "|    kl_mean_loss        | 18.4     |\n",
      "|    kl_std_loss         | 0.0346   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 69278    |\n",
      "|    penalty_temperature | 20.7     |\n",
      "|    policy_loss         | 55.1     |\n",
      "|    policy_mean_loss    | 28.9     |\n",
      "|    policy_std_loss     | 26.2     |\n",
      "|    std                 | 6.83     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 7.56e+10 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 173      |\n",
      "|    ep_rew_mean         | -124     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 372      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 1005     |\n",
      "|    total_timesteps     | 69938    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 9.75e+10 |\n",
      "|    alpha_mean          | 13.1     |\n",
      "|    alpha_mean_loss     | -17.6    |\n",
      "|    alpha_std           | 24.6     |\n",
      "|    alpha_std_loss      | -0.0336  |\n",
      "|    critic_loss         | 6.99e+21 |\n",
      "|    dual_loss           | 9.75e+10 |\n",
      "|    kl_loss             | 18.2     |\n",
      "|    kl_mean_loss        | 18.2     |\n",
      "|    kl_std_loss         | 0.0337   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 69654    |\n",
      "|    penalty_temperature | 20.8     |\n",
      "|    policy_loss         | 55.4     |\n",
      "|    policy_mean_loss    | 29.1     |\n",
      "|    policy_std_loss     | 26.3     |\n",
      "|    std                 | 7.01     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 9.75e+10 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 159      |\n",
      "|    ep_rew_mean         | -124     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 376      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 1013     |\n",
      "|    total_timesteps     | 70508    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 1.36e+11 |\n",
      "|    alpha_mean          | 13.2     |\n",
      "|    alpha_mean_loss     | -24.3    |\n",
      "|    alpha_std           | 24.7     |\n",
      "|    alpha_std_loss      | -0.0324  |\n",
      "|    critic_loss         | 1.37e+22 |\n",
      "|    dual_loss           | 1.36e+11 |\n",
      "|    kl_loss             | 24.9     |\n",
      "|    kl_mean_loss        | 24.8     |\n",
      "|    kl_std_loss         | 0.0325   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 70225    |\n",
      "|    penalty_temperature | 21       |\n",
      "|    policy_loss         | 56.8     |\n",
      "|    policy_mean_loss    | 30.3     |\n",
      "|    policy_std_loss     | 26.6     |\n",
      "|    std                 | 7.29     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 1.36e+11 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 163      |\n",
      "|    ep_rew_mean         | -124     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 380      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 1023     |\n",
      "|    total_timesteps     | 71186    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 1.97e+11 |\n",
      "|    alpha_mean          | 13.4     |\n",
      "|    alpha_mean_loss     | -20.6    |\n",
      "|    alpha_std           | 24.8     |\n",
      "|    alpha_std_loss      | -0.0324  |\n",
      "|    critic_loss         | 2.92e+22 |\n",
      "|    dual_loss           | 1.97e+11 |\n",
      "|    kl_loss             | 21.2     |\n",
      "|    kl_mean_loss        | 21.2     |\n",
      "|    kl_std_loss         | 0.0325   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 70892    |\n",
      "|    penalty_temperature | 21.2     |\n",
      "|    policy_loss         | 56.8     |\n",
      "|    policy_mean_loss    | 30       |\n",
      "|    policy_std_loss     | 26.8     |\n",
      "|    std                 | 7.65     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 1.97e+11 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 163      |\n",
      "|    ep_rew_mean         | -124     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 384      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 1029     |\n",
      "|    total_timesteps     | 71566    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 2.51e+11 |\n",
      "|    alpha_mean          | 13.5     |\n",
      "|    alpha_mean_loss     | -22.7    |\n",
      "|    alpha_std           | 24.9     |\n",
      "|    alpha_std_loss      | -0.0317  |\n",
      "|    critic_loss         | 4.8e+22  |\n",
      "|    dual_loss           | 2.51e+11 |\n",
      "|    kl_loss             | 23.2     |\n",
      "|    kl_mean_loss        | 23.2     |\n",
      "|    kl_std_loss         | 0.0318   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 71311    |\n",
      "|    penalty_temperature | 21.4     |\n",
      "|    policy_loss         | 57.5     |\n",
      "|    policy_mean_loss    | 30.5     |\n",
      "|    policy_std_loss     | 27.1     |\n",
      "|    std                 | 7.9      |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 2.51e+11 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 168      |\n",
      "|    ep_rew_mean         | -124     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 388      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 1038     |\n",
      "|    total_timesteps     | 72233    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 3.2e+11  |\n",
      "|    alpha_mean          | 13.7     |\n",
      "|    alpha_mean_loss     | -30.6    |\n",
      "|    alpha_std           | 25       |\n",
      "|    alpha_std_loss      | -0.0335  |\n",
      "|    critic_loss         | 7.92e+22 |\n",
      "|    dual_loss           | 3.2e+11  |\n",
      "|    kl_loss             | 31.1     |\n",
      "|    kl_mean_loss        | 31.1     |\n",
      "|    kl_std_loss         | 0.0336   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 71899    |\n",
      "|    penalty_temperature | 21.6     |\n",
      "|    policy_loss         | 59.1     |\n",
      "|    policy_mean_loss    | 31.8     |\n",
      "|    policy_std_loss     | 27.3     |\n",
      "|    std                 | 8.26     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 3.2e+11  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 155      |\n",
      "|    ep_rew_mean         | -124     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 392      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 1046     |\n",
      "|    total_timesteps     | 72718    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 3.98e+11 |\n",
      "|    alpha_mean          | 13.8     |\n",
      "|    alpha_mean_loss     | -29.5    |\n",
      "|    alpha_std           | 25       |\n",
      "|    alpha_std_loss      | -0.0343  |\n",
      "|    critic_loss         | 1.21e+23 |\n",
      "|    dual_loss           | 3.98e+11 |\n",
      "|    kl_loss             | 30.1     |\n",
      "|    kl_mean_loss        | 30       |\n",
      "|    kl_std_loss         | 0.0344   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 72440    |\n",
      "|    penalty_temperature | 21.8     |\n",
      "|    policy_loss         | 59.4     |\n",
      "|    policy_mean_loss    | 31.9     |\n",
      "|    policy_std_loss     | 27.5     |\n",
      "|    std                 | 8.63     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 3.98e+11 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 137      |\n",
      "|    ep_rew_mean         | -124     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 396      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 1052     |\n",
      "|    total_timesteps     | 73032    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 4.62e+11 |\n",
      "|    alpha_mean          | 13.9     |\n",
      "|    alpha_mean_loss     | -30.7    |\n",
      "|    alpha_std           | 25.1     |\n",
      "|    alpha_std_loss      | -0.0342  |\n",
      "|    critic_loss         | 1.69e+23 |\n",
      "|    dual_loss           | 4.62e+11 |\n",
      "|    kl_loss             | 31.3     |\n",
      "|    kl_mean_loss        | 31.3     |\n",
      "|    kl_std_loss         | 0.0343   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 72832    |\n",
      "|    penalty_temperature | 21.9     |\n",
      "|    policy_loss         | 59.9     |\n",
      "|    policy_mean_loss    | 32.2     |\n",
      "|    policy_std_loss     | 27.7     |\n",
      "|    std                 | 8.92     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 4.62e+11 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 136      |\n",
      "|    ep_rew_mean         | -124     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 400      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 1058     |\n",
      "|    total_timesteps     | 73595    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 5.44e+11 |\n",
      "|    alpha_mean          | 14       |\n",
      "|    alpha_mean_loss     | -32.1    |\n",
      "|    alpha_std           | 25.1     |\n",
      "|    alpha_std_loss      | -0.0357  |\n",
      "|    critic_loss         | 2.38e+23 |\n",
      "|    dual_loss           | 5.44e+11 |\n",
      "|    kl_loss             | 32.7     |\n",
      "|    kl_mean_loss        | 32.6     |\n",
      "|    kl_std_loss         | 0.0358   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 73282    |\n",
      "|    penalty_temperature | 22.1     |\n",
      "|    policy_loss         | 60.5     |\n",
      "|    policy_mean_loss    | 32.6     |\n",
      "|    policy_std_loss     | 27.9     |\n",
      "|    std                 | 9.27     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 5.44e+11 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 133      |\n",
      "|    ep_rew_mean         | -124     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 404      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 1065     |\n",
      "|    total_timesteps     | 74157    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 6.25e+11 |\n",
      "|    alpha_mean          | 14.2     |\n",
      "|    alpha_mean_loss     | -37.7    |\n",
      "|    alpha_std           | 25.2     |\n",
      "|    alpha_std_loss      | -0.0357  |\n",
      "|    critic_loss         | 3.12e+23 |\n",
      "|    dual_loss           | 6.25e+11 |\n",
      "|    kl_loss             | 38.3     |\n",
      "|    kl_mean_loss        | 38.3     |\n",
      "|    kl_std_loss         | 0.0358   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 73761    |\n",
      "|    penalty_temperature | 22.2     |\n",
      "|    policy_loss         | 61.7     |\n",
      "|    policy_mean_loss    | 33.5     |\n",
      "|    policy_std_loss     | 28.1     |\n",
      "|    std                 | 9.67     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 6.25e+11 |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/               |          |\n",
      "|    ep_len_mean         | 130      |\n",
      "|    ep_rew_mean         | -124     |\n",
      "| time/                  |          |\n",
      "|    episodes            | 408      |\n",
      "|    fps                 | 69       |\n",
      "|    time_elapsed        | 1075     |\n",
      "|    total_timesteps     | 74696    |\n",
      "| train/                 |          |\n",
      "|    actor_loss          | 7.4e+11  |\n",
      "|    alpha_mean          | 14.4     |\n",
      "|    alpha_mean_loss     | -38.1    |\n",
      "|    alpha_std           | 25.2     |\n",
      "|    alpha_std_loss      | -0.0373  |\n",
      "|    critic_loss         | 4.39e+23 |\n",
      "|    dual_loss           | 7.4e+11  |\n",
      "|    kl_loss             | 38.7     |\n",
      "|    kl_mean_loss        | 38.7     |\n",
      "|    kl_std_loss         | 0.0374   |\n",
      "|    learning_rate       | 0.0003   |\n",
      "|    n_updates           | 74442    |\n",
      "|    penalty_temperature | 22.5     |\n",
      "|    policy_loss         | 62.3     |\n",
      "|    policy_mean_loss    | 33.8     |\n",
      "|    policy_std_loss     | 28.5     |\n",
      "|    std                 | 10.3     |\n",
      "|    temperature         | 11.5     |\n",
      "|    temperature_loss    | 7.4e+11  |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jankaiser/Documents/DESY/stable-baselines3-contrib/test_mpo.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jankaiser/Documents/DESY/stable-baselines3-contrib/test_mpo.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m MPO(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jankaiser/Documents/DESY/stable-baselines3-contrib/test_mpo.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     policy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jankaiser/Documents/DESY/stable-baselines3-contrib/test_mpo.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     env\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBipedalWalker-v3\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jankaiser/Documents/DESY/stable-baselines3-contrib/test_mpo.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     num_samples\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jankaiser/Documents/DESY/stable-baselines3-contrib/test_mpo.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jankaiser/Documents/DESY/stable-baselines3-contrib/test_mpo.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m2_000_000\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/DESY/stable-baselines3-contrib/sb3_contrib/mpo/mpo.py:382\u001b[0m, in \u001b[0;36mMPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[39mself\u001b[39m: SelfMPO,\n\u001b[1;32m    375\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    381\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfMPO:\n\u001b[0;32m--> 382\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    383\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    384\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    385\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    386\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    387\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    388\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    389\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/stable-baselines3-contrib/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:331\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    330\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 331\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[1;32m    333\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    335\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/DESY/stable-baselines3-contrib/sb3_contrib/mpo/mpo.py:230\u001b[0m, in \u001b[0;36mMPO.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_penalty_temperature\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclamp_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_log_dual, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    227\u001b[0m target_distributions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_target\u001b[39m.\u001b[39mpredict_action_distribution(\n\u001b[1;32m    228\u001b[0m     replay_data\u001b[39m.\u001b[39mnext_observations\n\u001b[1;32m    229\u001b[0m )\u001b[39m.\u001b[39mdistribution\n\u001b[0;32m--> 230\u001b[0m next_action_samples \u001b[39m=\u001b[39m target_distributions\u001b[39m.\u001b[39;49msample((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_samples,))\n\u001b[1;32m    232\u001b[0m tiled_observations \u001b[39m=\u001b[39m replay_data\u001b[39m.\u001b[39mobservations\u001b[39m.\u001b[39mtile(\n\u001b[1;32m    233\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples,\n\u001b[1;32m    234\u001b[0m     \u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(replay_data\u001b[39m.\u001b[39mobservations\u001b[39m.\u001b[39mdim())),\n\u001b[1;32m    235\u001b[0m )\n\u001b[1;32m    236\u001b[0m tiled_next_observations \u001b[39m=\u001b[39m replay_data\u001b[39m.\u001b[39mnext_observations\u001b[39m.\u001b[39mtile(\n\u001b[1;32m    237\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples,\n\u001b[1;32m    238\u001b[0m     \u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(replay_data\u001b[39m.\u001b[39mnext_observations\u001b[39m.\u001b[39mdim())),\n\u001b[1;32m    239\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/stable-baselines3-contrib/lib/python3.9/site-packages/torch/distributions/normal.py:70\u001b[0m, in \u001b[0;36mNormal.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m     68\u001b[0m shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extended_shape(sample_shape)\n\u001b[1;32m     69\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mnormal(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc\u001b[39m.\u001b[39;49mexpand(shape), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale\u001b[39m.\u001b[39;49mexpand(shape))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=\"BipedalWalker-v3\",\n",
    "    verbose=2,\n",
    "    tensorboard_log=\"./logs\",\n",
    "    epsilon_mean=0.01,\n",
    "    policy_kwargs={\"net_arch\": [256, 256, 256], \"log_std_init\": np.log(0.5)},\n",
    "    batch_size=256,\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    learning_starts=1_000,\n",
    "    buffer_size=1_000_000,\n",
    "    num_samples=20,\n",
    ")\n",
    "model.learn(total_timesteps=2_000_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable-baselines3-contrib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
